{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from utils.tools import get_config, default_loader, is_image_file, normalize\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "sys.path.append('../PerceptualSimilarity')\n",
    "import models as PerceptualSimilarity\n",
    "\n",
    "# personal library\n",
    "from networks import autoencoder, simulator, discriminator\n",
    "from dataloader import MVTecDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 限制可以使用的 GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER parameters\n",
    "num_epochs = 50000\n",
    "batch_size = 32\n",
    "val_batch_size = 4\n",
    "ae_lr = 1e-4\n",
    "s_lr = 5e-4\n",
    "d_lr = 5e-4\n",
    "weight_decay = 1e-5\n",
    "UPSET=True\n",
    "expName = 'AEGAN-exp(wood + L1)'\n",
    "writer = SummaryWriter('checkpoints/'+expName)\n",
    "TYPE='wood'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDatset = MVTecDataset.MVTecDataset(TYPE=TYPE, isTrain='train')\n",
    "testDatset = MVTecDataset.MVTecDataset(TYPE=TYPE, isTrain='test')\n",
    "valDataset = MVTecDataset.MVTecDataset(TYPE=TYPE, isTrain='val')\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=valDataset,\n",
    "    batch_size=val_batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset=trainDatset,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=testDatset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Perceptual loss...\n",
      "Loading model from: /root/AFS/Corn/AEGAN/PerceptualSimilarity/models/weights/v0.1/alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "AE = autoencoder.Autoencoder().cuda()\n",
    "S = nn.DataParallel(simulator.Simulator(3, 8)).cuda()\n",
    "D = nn.DataParallel(discriminator.Discriminator(6, 16)).cuda()\n",
    "\n",
    "# Loss\n",
    "L1_loss = nn.L1Loss()\n",
    "L2_loss = nn.MSELoss(reduction='none')\n",
    "perceptual_loss = PerceptualSimilarity.PerceptualLoss(model='net-lin', net='alex', use_gpu=True, gpu_ids=[0])\n",
    "\n",
    "# Optimizer\n",
    "optimizer_AE = torch.optim.Adam(\n",
    "    AE.parameters(), \n",
    "    lr=ae_lr,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "optimizer_S = torch.optim.Adam(\n",
    "    S.parameters(), \n",
    "    lr=s_lr,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "optimizer_D = torch.optim.Adam(\n",
    "    D.parameters(), \n",
    "    lr=d_lr,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.load_state_dict(torch.load('./save_weight/AE-wood-z-2x2-exp2/AE_2500.npy', map_location=\"cuda:0\"), False)\n",
    "\n",
    "AE = nn.DataParallel(AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Solve: RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED\n",
    "torch.backends.cudnn.enabled = False \n",
    "\n",
    "# 拿掉煩人的 warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "\n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    # print \"real_data: \", real_data.size(), fake_data.size()\n",
    "    BATCH_SIZE = real_data.size(0)\n",
    "    alpha = torch.rand(BATCH_SIZE, 1)\n",
    "    alpha = alpha.expand(BATCH_SIZE, real_data.nelement()//BATCH_SIZE).contiguous().view(BATCH_SIZE, 6, 256, 256)\n",
    "    alpha = alpha.cuda()\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = autograd.grad(\n",
    "        outputs=disc_interpolates, \n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "        create_graph=True, \n",
    "        retain_graph=True, \n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "    return gradient_penalty\n",
    "\n",
    "def difNormalize(input_matrix, threshold=None):\n",
    "    _min = torch.min(input_matrix)\n",
    "    _max = torch.max(input_matrix)\n",
    "    \n",
    "    input_matrix = (input_matrix - _min) / (_max - _min)\n",
    "    \n",
    "    if threshold != None:\n",
    "        input_matrix[input_matrix < threshold] = 0\n",
    "        input_matrix[input_matrix >= threshold] = 1\n",
    "        \n",
    "    return input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/50000] s_loss:1883.6453 d_loss:-219.3843 val_s_loss:507.6328 val_d_loss:418.7781 cost:25.63\n",
      "epoch [2/50000] s_loss:1597.4044 d_loss:-225.0609 val_s_loss:877.4207 val_d_loss:-70.1391 cost:21.18\n",
      "epoch [3/50000] s_loss:2224.2908 d_loss:-198.5246 val_s_loss:1143.3422 val_d_loss:-145.5718 cost:21.81\n",
      "epoch [4/50000] s_loss:1453.7805 d_loss:-147.2471 val_s_loss:726.8510 val_d_loss:-48.4474 cost:22.60\n",
      "epoch [5/50000] s_loss:1204.1757 d_loss:-115.5312 val_s_loss:612.3178 val_d_loss:-87.9609 cost:21.96\n",
      "epoch [6/50000] s_loss:856.0828 d_loss:-105.1086 val_s_loss:401.2988 val_d_loss:-46.0550 cost:21.99\n",
      "epoch [7/50000] s_loss:700.2130 d_loss:-116.1161 val_s_loss:432.7015 val_d_loss:-157.3034 cost:21.50\n",
      "epoch [8/50000] s_loss:613.8437 d_loss:-133.3876 val_s_loss:944.3091 val_d_loss:-567.8423 cost:24.36\n",
      "epoch [9/50000] s_loss:486.1114 d_loss:-94.5679 val_s_loss:388.3938 val_d_loss:-197.6541 cost:24.84\n",
      "epoch [10/50000] s_loss:438.2088 d_loss:-122.0010 val_s_loss:195.7018 val_d_loss:73.6540 cost:24.10\n",
      "epoch [11/50000] s_loss:355.0572 d_loss:-60.6012 val_s_loss:111.5175 val_d_loss:7.5777 cost:23.80\n",
      "epoch [12/50000] s_loss:685.8068 d_loss:-59.7895 val_s_loss:450.9721 val_d_loss:-114.8758 cost:23.69\n",
      "epoch [13/50000] s_loss:303.9009 d_loss:-65.7777 val_s_loss:80.4307 val_d_loss:6.9492 cost:24.30\n",
      "epoch [14/50000] s_loss:113.8153 d_loss:-11.4138 val_s_loss:82.0507 val_d_loss:-37.4349 cost:22.51\n",
      "epoch [15/50000] s_loss:264.6172 d_loss:-15.0950 val_s_loss:157.9728 val_d_loss:-43.6503 cost:23.27\n",
      "epoch [16/50000] s_loss:399.5502 d_loss:-40.1807 val_s_loss:167.5791 val_d_loss:-1.0890 cost:22.75\n",
      "epoch [17/50000] s_loss:394.7859 d_loss:-26.6052 val_s_loss:390.7823 val_d_loss:-202.0413 cost:25.34\n",
      "epoch [18/50000] s_loss:161.5410 d_loss:-23.9069 val_s_loss:176.7704 val_d_loss:-109.6653 cost:26.32\n",
      "epoch [19/50000] s_loss:115.3571 d_loss:-4.9595 val_s_loss:100.9721 val_d_loss:-50.8192 cost:27.24\n",
      "epoch [20/50000] s_loss:308.4601 d_loss:-18.7860 val_s_loss:163.0697 val_d_loss:-19.4201 cost:24.46\n",
      "epoch [21/50000] s_loss:373.1725 d_loss:-12.5963 val_s_loss:195.1534 val_d_loss:-28.3979 cost:24.20\n",
      "epoch [22/50000] s_loss:450.3647 d_loss:-18.8614 val_s_loss:202.9894 val_d_loss:10.0757 cost:24.31\n",
      "epoch [23/50000] s_loss:383.1512 d_loss:-12.5366 val_s_loss:163.9712 val_d_loss:2.3556 cost:24.22\n",
      "epoch [24/50000] s_loss:306.9857 d_loss:-31.8980 val_s_loss:121.5393 val_d_loss:4.6655 cost:21.88\n",
      "epoch [25/50000] s_loss:404.6372 d_loss:-19.8163 val_s_loss:181.0411 val_d_loss:6.0931 cost:21.83\n",
      "epoch [26/50000] s_loss:334.5571 d_loss:-19.0403 val_s_loss:195.8179 val_d_loss:-50.7673 cost:22.19\n",
      "epoch [27/50000] s_loss:505.6439 d_loss:-27.1844 val_s_loss:226.3889 val_d_loss:10.1082 cost:21.93\n",
      "epoch [28/50000] s_loss:338.7038 d_loss:-24.4412 val_s_loss:124.4940 val_d_loss:2.3804 cost:24.50\n",
      "epoch [29/50000] s_loss:419.9118 d_loss:-37.8772 val_s_loss:169.6190 val_d_loss:2.2505 cost:23.64\n",
      "epoch [30/50000] s_loss:210.6303 d_loss:-20.2024 val_s_loss:82.4579 val_d_loss:-0.8691 cost:22.85\n",
      "epoch [31/50000] s_loss:403.5631 d_loss:-5.4561 val_s_loss:179.3227 val_d_loss:-14.6635 cost:22.23\n",
      "epoch [32/50000] s_loss:350.9449 d_loss:-29.0553 val_s_loss:152.6981 val_d_loss:-7.9780 cost:21.66\n",
      "epoch [33/50000] s_loss:130.5721 d_loss:-21.6007 val_s_loss:52.4450 val_d_loss:-18.8631 cost:22.74\n",
      "epoch [34/50000] s_loss:244.0636 d_loss:-22.2167 val_s_loss:100.2063 val_d_loss:-0.4914 cost:22.06\n",
      "epoch [35/50000] s_loss:228.8009 d_loss:-31.6062 val_s_loss:441.1084 val_d_loss:-300.8669 cost:21.94\n",
      "epoch [36/50000] s_loss:175.2686 d_loss:-14.8074 val_s_loss:123.3442 val_d_loss:-61.3274 cost:21.75\n",
      "epoch [37/50000] s_loss:205.7842 d_loss:-35.8354 val_s_loss:69.3012 val_d_loss:0.2561 cost:21.96\n",
      "epoch [38/50000] s_loss:171.7506 d_loss:-27.2973 val_s_loss:266.3823 val_d_loss:-178.3181 cost:24.77\n",
      "epoch [39/50000] s_loss:157.6791 d_loss:-25.6556 val_s_loss:68.1422 val_d_loss:-14.4247 cost:24.11\n",
      "epoch [40/50000] s_loss:176.2671 d_loss:-28.0210 val_s_loss:47.8665 val_d_loss:0.7081 cost:22.93\n",
      "epoch [41/50000] s_loss:109.6185 d_loss:-21.2936 val_s_loss:38.3473 val_d_loss:-10.6272 cost:22.99\n",
      "epoch [42/50000] s_loss:142.2297 d_loss:-26.5040 val_s_loss:44.4271 val_d_loss:-1.1153 cost:21.81\n",
      "epoch [43/50000] s_loss:180.0275 d_loss:-16.3020 val_s_loss:103.5935 val_d_loss:-29.6874 cost:22.74\n",
      "epoch [44/50000] s_loss:177.6840 d_loss:-15.2289 val_s_loss:160.3100 val_d_loss:-83.8893 cost:22.39\n",
      "epoch [45/50000] s_loss:92.8862 d_loss:-22.8853 val_s_loss:31.6353 val_d_loss:-5.6543 cost:21.79\n",
      "epoch [46/50000] s_loss:113.6158 d_loss:-26.6812 val_s_loss:74.5642 val_d_loss:-41.2377 cost:21.50\n",
      "epoch [47/50000] s_loss:247.4015 d_loss:-25.9988 val_s_loss:117.2707 val_d_loss:-32.3929 cost:21.84\n",
      "epoch [48/50000] s_loss:172.0665 d_loss:-31.7118 val_s_loss:53.1659 val_d_loss:-1.0034 cost:25.09\n",
      "epoch [49/50000] s_loss:73.2720 d_loss:-24.3785 val_s_loss:583.1740 val_d_loss:-457.4309 cost:23.73\n",
      "epoch [50/50000] s_loss:234.1360 d_loss:-23.0648 val_s_loss:184.3879 val_d_loss:-68.2229 cost:22.26\n",
      "epoch [51/50000] s_loss:99.1509 d_loss:-19.0772 val_s_loss:28.0147 val_d_loss:-5.0845 cost:22.28\n",
      "epoch [52/50000] s_loss:18.3066 d_loss:-21.9247 val_s_loss:746.6213 val_d_loss:-611.4149 cost:22.29\n",
      "epoch [53/50000] s_loss:397.7733 d_loss:-17.5189 val_s_loss:324.4878 val_d_loss:-127.2691 cost:23.44\n",
      "epoch [54/50000] s_loss:378.8634 d_loss:-6.1809 val_s_loss:194.3159 val_d_loss:-17.1518 cost:22.24\n",
      "epoch [55/50000] s_loss:349.4015 d_loss:-14.1630 val_s_loss:174.9336 val_d_loss:-17.7551 cost:22.20\n",
      "epoch [56/50000] s_loss:333.2813 d_loss:-20.6993 val_s_loss:156.6144 val_d_loss:2.6433 cost:22.38\n",
      "epoch [57/50000] s_loss:302.9925 d_loss:-15.0206 val_s_loss:137.3898 val_d_loss:1.8363 cost:22.72\n",
      "epoch [58/50000] s_loss:275.6639 d_loss:-17.6161 val_s_loss:116.6142 val_d_loss:6.6322 cost:24.04\n",
      "epoch [59/50000] s_loss:205.7660 d_loss:-14.2381 val_s_loss:76.8052 val_d_loss:1.4912 cost:22.75\n",
      "epoch [60/50000] s_loss:284.1766 d_loss:-18.4636 val_s_loss:127.9236 val_d_loss:-0.9362 cost:22.13\n",
      "epoch [61/50000] s_loss:255.8578 d_loss:-18.0401 val_s_loss:163.9536 val_d_loss:-45.3373 cost:22.12\n",
      "epoch [62/50000] s_loss:219.7345 d_loss:-20.9149 val_s_loss:132.4547 val_d_loss:-35.4934 cost:22.80\n",
      "epoch [63/50000] s_loss:118.9415 d_loss:-16.5352 val_s_loss:51.9710 val_d_loss:-3.8780 cost:22.58\n",
      "epoch [64/50000] s_loss:143.2196 d_loss:-19.2074 val_s_loss:63.5666 val_d_loss:-8.9835 cost:22.48\n",
      "epoch [65/50000] s_loss:137.8885 d_loss:-23.0026 val_s_loss:599.7532 val_d_loss:-445.5069 cost:22.29\n",
      "epoch [66/50000] s_loss:117.1573 d_loss:-20.3601 val_s_loss:47.1965 val_d_loss:-2.1975 cost:22.39\n",
      "epoch [67/50000] s_loss:123.0866 d_loss:-24.7042 val_s_loss:48.4284 val_d_loss:1.9597 cost:23.39\n",
      "epoch [68/50000] s_loss:202.6806 d_loss:-21.1851 val_s_loss:76.0022 val_d_loss:4.4957 cost:24.96\n",
      "epoch [69/50000] s_loss:164.2394 d_loss:-17.6310 val_s_loss:66.7598 val_d_loss:-3.1101 cost:24.10\n",
      "epoch [70/50000] s_loss:109.6826 d_loss:-19.3320 val_s_loss:42.1891 val_d_loss:-6.5932 cost:23.14\n",
      "epoch [71/50000] s_loss:832.9908 d_loss:-101.6408 val_s_loss:1354.5171 val_d_loss:-915.9113 cost:22.87\n",
      "epoch [72/50000] s_loss:64.7020 d_loss:1.4010 val_s_loss:44.3049 val_d_loss:-10.9025 cost:23.74\n",
      "epoch [73/50000] s_loss:93.4371 d_loss:-1.9882 val_s_loss:54.3055 val_d_loss:-8.9507 cost:22.88\n",
      "epoch [74/50000] s_loss:153.8902 d_loss:-6.8047 val_s_loss:64.9889 val_d_loss:4.8354 cost:21.98\n",
      "epoch [75/50000] s_loss:182.4925 d_loss:-14.7263 val_s_loss:74.5020 val_d_loss:4.0605 cost:22.40\n",
      "epoch [76/50000] s_loss:151.9814 d_loss:-22.4334 val_s_loss:73.1954 val_d_loss:-11.8267 cost:22.15\n",
      "epoch [77/50000] s_loss:74.9231 d_loss:-8.9567 val_s_loss:191.5917 val_d_loss:-132.0566 cost:24.00\n",
      "epoch [78/50000] s_loss:389.4938 d_loss:-20.8407 val_s_loss:552.3348 val_d_loss:-366.6891 cost:24.94\n",
      "epoch [79/50000] s_loss:792.0208 d_loss:-76.9229 val_s_loss:726.8302 val_d_loss:-259.0345 cost:22.96\n",
      "epoch [80/50000] s_loss:270.0512 d_loss:-76.4523 val_s_loss:751.5155 val_d_loss:-543.8126 cost:22.86\n",
      "epoch [81/50000] s_loss:285.6750 d_loss:-41.3840 val_s_loss:563.7946 val_d_loss:-392.7115 cost:23.48\n",
      "epoch [82/50000] s_loss:233.2363 d_loss:-28.1328 val_s_loss:239.9583 val_d_loss:-119.6857 cost:24.94\n",
      "epoch [83/50000] s_loss:122.7347 d_loss:-15.0309 val_s_loss:25.4723 val_d_loss:0.3482 cost:23.66\n",
      "epoch [84/50000] s_loss:133.3554 d_loss:-24.9245 val_s_loss:43.0681 val_d_loss:0.4914 cost:23.45\n",
      "epoch [85/50000] s_loss:122.7990 d_loss:-15.1483 val_s_loss:42.9006 val_d_loss:-7.5227 cost:22.69\n",
      "epoch [86/50000] s_loss:119.3311 d_loss:-23.1474 val_s_loss:43.4822 val_d_loss:-2.3081 cost:22.71\n",
      "epoch [87/50000] s_loss:98.2233 d_loss:-16.0787 val_s_loss:30.7523 val_d_loss:1.7921 cost:26.44\n",
      "epoch [88/50000] s_loss:147.0295 d_loss:-16.7595 val_s_loss:44.8343 val_d_loss:0.0837 cost:24.01\n",
      "epoch [89/50000] s_loss:42.3172 d_loss:-27.9021 val_s_loss:1.3402 val_d_loss:-1.7834 cost:23.27\n",
      "epoch [90/50000] s_loss:5.1631 d_loss:-17.1728 val_s_loss:-16.0181 val_d_loss:0.9983 cost:22.74\n",
      "epoch [91/50000] s_loss:-50.6202 d_loss:-10.0332 val_s_loss:49.1562 val_d_loss:-62.9933 cost:22.44\n",
      "epoch [92/50000] s_loss:233.3271 d_loss:-5.9142 val_s_loss:106.7694 val_d_loss:-1.8989 cost:23.37\n",
      "epoch [93/50000] s_loss:-20.1453 d_loss:-9.5428 val_s_loss:-20.2520 val_d_loss:0.4769 cost:22.02\n",
      "epoch [94/50000] s_loss:50.4667 d_loss:-11.9653 val_s_loss:34.3351 val_d_loss:-13.4847 cost:22.14\n",
      "epoch [95/50000] s_loss:201.9033 d_loss:-15.9524 val_s_loss:129.1597 val_d_loss:-38.6765 cost:22.52\n",
      "epoch [96/50000] s_loss:59.1987 d_loss:-17.8678 val_s_loss:19.6460 val_d_loss:-2.0881 cost:23.67\n",
      "epoch [97/50000] s_loss:21.4801 d_loss:-15.9104 val_s_loss:-4.6555 val_d_loss:-0.8161 cost:25.84\n",
      "epoch [98/50000] s_loss:-38.9155 d_loss:-16.9460 val_s_loss:-20.2697 val_d_loss:-13.8826 cost:23.88\n",
      "epoch [99/50000] s_loss:-38.5061 d_loss:-17.0720 val_s_loss:309.7255 val_d_loss:-252.4066 cost:23.80\n",
      "epoch [100/50000] s_loss:-71.6036 d_loss:-26.6131 val_s_loss:29.2967 val_d_loss:-71.5688 cost:23.66\n",
      "epoch [101/50000] s_loss:-31.6777 d_loss:-12.4335 val_s_loss:-9.3014 val_d_loss:-25.7089 cost:24.29\n",
      "epoch [102/50000] s_loss:-86.0540 d_loss:-23.8843 val_s_loss:-60.9369 val_d_loss:0.0139 cost:23.10\n",
      "epoch [103/50000] s_loss:-82.2736 d_loss:-11.9379 val_s_loss:-65.1967 val_d_loss:4.6017 cost:23.40\n",
      "epoch [104/50000] s_loss:-32.9378 d_loss:-7.0879 val_s_loss:-31.6128 val_d_loss:1.6127 cost:22.84\n",
      "epoch [105/50000] s_loss:51.7462 d_loss:-13.6249 val_s_loss:55.7369 val_d_loss:-39.6110 cost:22.35\n",
      "epoch [106/50000] s_loss:-14.9395 d_loss:-13.4938 val_s_loss:19.6117 val_d_loss:-36.1651 cost:24.05\n",
      "epoch [107/50000] s_loss:-132.0079 d_loss:-13.7272 val_s_loss:-77.9358 val_d_loss:-0.9640 cost:24.68\n",
      "epoch [108/50000] s_loss:95.1649 d_loss:-9.0846 val_s_loss:35.9997 val_d_loss:1.9326 cost:23.43\n",
      "epoch [109/50000] s_loss:52.2533 d_loss:-10.7793 val_s_loss:52.7492 val_d_loss:-31.3831 cost:22.54\n",
      "epoch [110/50000] s_loss:-63.4027 d_loss:-18.6047 val_s_loss:-26.3075 val_d_loss:-17.6904 cost:22.24\n",
      "epoch [111/50000] s_loss:-57.2606 d_loss:-15.2697 val_s_loss:-40.2480 val_d_loss:-0.4288 cost:24.16\n",
      "epoch [112/50000] s_loss:-75.9741 d_loss:-9.7946 val_s_loss:-48.1532 val_d_loss:-2.8620 cost:22.30\n",
      "epoch [113/50000] s_loss:-128.9525 d_loss:-16.0438 val_s_loss:-81.4511 val_d_loss:0.7109 cost:22.48\n",
      "epoch [114/50000] s_loss:-39.9582 d_loss:-12.2664 val_s_loss:-35.1118 val_d_loss:5.6852 cost:22.41\n",
      "epoch [115/50000] s_loss:52.8133 d_loss:-9.0424 val_s_loss:19.8994 val_d_loss:-5.4493 cost:22.19\n",
      "epoch [116/50000] s_loss:-2.2266 d_loss:-4.7647 val_s_loss:44.6394 val_d_loss:-44.2508 cost:24.35\n",
      "epoch [117/50000] s_loss:-29.2981 d_loss:-4.2737 val_s_loss:-20.4058 val_d_loss:2.6969 cost:24.22\n",
      "epoch [118/50000] s_loss:57.9437 d_loss:-6.4830 val_s_loss:29.7106 val_d_loss:-5.8648 cost:23.29\n",
      "epoch [119/50000] s_loss:52.6854 d_loss:-3.6221 val_s_loss:170.3907 val_d_loss:-136.3488 cost:22.98\n",
      "epoch [120/50000] s_loss:218.0584 d_loss:-7.3170 val_s_loss:374.6655 val_d_loss:-263.1080 cost:22.89\n",
      "epoch [121/50000] s_loss:328.3654 d_loss:-25.5899 val_s_loss:358.9138 val_d_loss:-189.0207 cost:23.44\n",
      "epoch [122/50000] s_loss:240.3600 d_loss:-13.6718 val_s_loss:85.5951 val_d_loss:7.9304 cost:22.46\n",
      "epoch [123/50000] s_loss:142.4578 d_loss:-30.7598 val_s_loss:69.7113 val_d_loss:-0.7851 cost:22.30\n",
      "epoch [124/50000] s_loss:871.0288 d_loss:-144.5071 val_s_loss:3327.1006 val_d_loss:-2102.4119 cost:22.07\n",
      "epoch [125/50000] s_loss:270.6223 d_loss:-30.6024 val_s_loss:637.5621 val_d_loss:-504.4685 cost:21.96\n",
      "epoch [126/50000] s_loss:235.2472 d_loss:-27.3889 val_s_loss:190.6135 val_d_loss:-92.8936 cost:25.63\n",
      "epoch [127/50000] s_loss:389.9251 d_loss:-29.9650 val_s_loss:192.3451 val_d_loss:-18.6679 cost:24.31\n",
      "epoch [128/50000] s_loss:375.4030 d_loss:-18.3197 val_s_loss:165.8133 val_d_loss:-10.9547 cost:23.56\n",
      "epoch [129/50000] s_loss:297.4626 d_loss:-26.1464 val_s_loss:120.4954 val_d_loss:-4.9012 cost:22.72\n",
      "epoch [130/50000] s_loss:212.7194 d_loss:-18.9904 val_s_loss:81.8765 val_d_loss:-4.5079 cost:22.86\n",
      "epoch [131/50000] s_loss:195.6158 d_loss:-20.7626 val_s_loss:80.2483 val_d_loss:0.2836 cost:23.16\n",
      "epoch [132/50000] s_loss:184.9152 d_loss:-19.2682 val_s_loss:82.1778 val_d_loss:-7.5918 cost:22.27\n",
      "epoch [133/50000] s_loss:67.4542 d_loss:-19.1566 val_s_loss:19.8288 val_d_loss:-5.0059 cost:22.32\n",
      "epoch [134/50000] s_loss:97.9795 d_loss:-14.7862 val_s_loss:55.2674 val_d_loss:-16.0835 cost:21.91\n",
      "epoch [135/50000] s_loss:111.9490 d_loss:-14.2238 val_s_loss:50.5333 val_d_loss:-9.1606 cost:22.70\n",
      "epoch [136/50000] s_loss:151.3883 d_loss:-13.1491 val_s_loss:59.5500 val_d_loss:-1.0362 cost:24.84\n",
      "epoch [137/50000] s_loss:196.2300 d_loss:-13.6306 val_s_loss:83.4271 val_d_loss:0.4866 cost:22.96\n",
      "epoch [138/50000] s_loss:203.2657 d_loss:-22.0059 val_s_loss:129.8697 val_d_loss:-32.6793 cost:22.47\n",
      "epoch [139/50000] s_loss:197.8984 d_loss:-21.0237 val_s_loss:84.2895 val_d_loss:0.0099 cost:22.86\n",
      "epoch [140/50000] s_loss:126.7102 d_loss:-18.1166 val_s_loss:49.9158 val_d_loss:-5.8083 cost:22.09\n",
      "epoch [141/50000] s_loss:150.5097 d_loss:-18.6971 val_s_loss:59.4878 val_d_loss:-20.5058 cost:22.86\n",
      "epoch [142/50000] s_loss:105.3992 d_loss:-14.6643 val_s_loss:58.3026 val_d_loss:-22.9122 cost:21.95\n",
      "epoch [143/50000] s_loss:83.7051 d_loss:-19.2744 val_s_loss:76.9993 val_d_loss:-32.0048 cost:21.96\n",
      "epoch [144/50000] s_loss:168.4317 d_loss:-20.0191 val_s_loss:67.3753 val_d_loss:-5.2143 cost:21.96\n",
      "epoch [145/50000] s_loss:103.9356 d_loss:-18.9581 val_s_loss:71.1198 val_d_loss:-40.6363 cost:22.74\n",
      "epoch [146/50000] s_loss:101.7165 d_loss:-25.0310 val_s_loss:41.6050 val_d_loss:-0.7036 cost:24.41\n",
      "epoch [147/50000] s_loss:140.7395 d_loss:-16.4639 val_s_loss:54.7182 val_d_loss:0.5618 cost:23.32\n",
      "epoch [148/50000] s_loss:104.3274 d_loss:-15.0889 val_s_loss:59.3932 val_d_loss:-31.6762 cost:23.01\n",
      "epoch [149/50000] s_loss:68.3485 d_loss:-14.7486 val_s_loss:120.4772 val_d_loss:-85.5170 cost:22.41\n",
      "epoch [150/50000] s_loss:121.9296 d_loss:-19.5864 val_s_loss:86.5260 val_d_loss:-39.0535 cost:22.23\n",
      "epoch [151/50000] s_loss:109.6061 d_loss:-27.5396 val_s_loss:48.7728 val_d_loss:-11.2699 cost:23.67\n",
      "epoch [152/50000] s_loss:67.6230 d_loss:-21.3940 val_s_loss:21.2791 val_d_loss:-1.2026 cost:22.71\n",
      "epoch [153/50000] s_loss:87.1166 d_loss:-18.6508 val_s_loss:42.8571 val_d_loss:-19.7238 cost:21.87\n",
      "epoch [154/50000] s_loss:212.9093 d_loss:-18.7557 val_s_loss:405.8049 val_d_loss:-237.6066 cost:22.20\n",
      "epoch [155/50000] s_loss:271.1325 d_loss:-11.1326 val_s_loss:1302.6152 val_d_loss:-985.9089 cost:21.66\n",
      "epoch [156/50000] s_loss:235.0399 d_loss:-30.4162 val_s_loss:94.5622 val_d_loss:-0.4565 cost:23.88\n",
      "epoch [157/50000] s_loss:174.4098 d_loss:-27.1157 val_s_loss:58.2815 val_d_loss:2.1293 cost:23.47\n",
      "epoch [158/50000] s_loss:93.6473 d_loss:-22.4741 val_s_loss:61.9512 val_d_loss:-23.5145 cost:22.25\n",
      "epoch [159/50000] s_loss:56.7881 d_loss:-19.6870 val_s_loss:36.0746 val_d_loss:-26.7277 cost:22.23\n",
      "epoch [160/50000] s_loss:92.6761 d_loss:-22.2612 val_s_loss:55.8699 val_d_loss:-11.0791 cost:22.47\n",
      "epoch [161/50000] s_loss:93.6608 d_loss:-18.1921 val_s_loss:39.7095 val_d_loss:-6.5752 cost:22.63\n",
      "epoch [162/50000] s_loss:96.1251 d_loss:-22.3187 val_s_loss:37.9306 val_d_loss:-11.9152 cost:22.44\n",
      "epoch [163/50000] s_loss:70.9145 d_loss:-19.2379 val_s_loss:5.1708 val_d_loss:14.5682 cost:22.16\n",
      "epoch [164/50000] s_loss:51.1768 d_loss:-14.0471 val_s_loss:25.4628 val_d_loss:-2.7542 cost:22.25\n",
      "epoch [165/50000] s_loss:159.7187 d_loss:-13.5982 val_s_loss:67.3331 val_d_loss:0.9314 cost:22.90\n",
      "epoch [166/50000] s_loss:233.6374 d_loss:-15.2668 val_s_loss:93.7198 val_d_loss:1.2266 cost:25.19\n",
      "epoch [167/50000] s_loss:254.7186 d_loss:-14.6712 val_s_loss:112.6763 val_d_loss:0.7178 cost:22.18\n",
      "epoch [168/50000] s_loss:163.4554 d_loss:-15.7210 val_s_loss:108.7820 val_d_loss:-28.6570 cost:22.26\n",
      "epoch [169/50000] s_loss:130.9144 d_loss:-26.1266 val_s_loss:125.5624 val_d_loss:-49.9548 cost:22.48\n",
      "epoch [170/50000] s_loss:139.7209 d_loss:-8.3303 val_s_loss:57.5953 val_d_loss:-3.0998 cost:23.07\n",
      "epoch [171/50000] s_loss:99.8134 d_loss:-13.5476 val_s_loss:39.9748 val_d_loss:-3.1907 cost:22.09\n",
      "epoch [172/50000] s_loss:107.0618 d_loss:-23.9263 val_s_loss:43.2831 val_d_loss:-2.0785 cost:21.83\n",
      "epoch [173/50000] s_loss:112.5597 d_loss:-17.0095 val_s_loss:43.1027 val_d_loss:-3.2823 cost:21.65\n",
      "epoch [174/50000] s_loss:105.4318 d_loss:-19.8013 val_s_loss:47.1563 val_d_loss:-10.1589 cost:21.80\n",
      "epoch [175/50000] s_loss:124.9157 d_loss:-16.0677 val_s_loss:48.1020 val_d_loss:-7.3578 cost:24.41\n",
      "epoch [176/50000] s_loss:85.2747 d_loss:-16.9184 val_s_loss:35.4695 val_d_loss:-4.6797 cost:24.62\n",
      "epoch [177/50000] s_loss:104.1057 d_loss:-12.2376 val_s_loss:107.5266 val_d_loss:-53.6364 cost:22.74\n",
      "epoch [178/50000] s_loss:214.8676 d_loss:-11.9712 val_s_loss:93.8546 val_d_loss:-3.1395 cost:22.44\n",
      "epoch [179/50000] s_loss:265.2815 d_loss:-22.1127 val_s_loss:120.8353 val_d_loss:-7.0289 cost:22.37\n",
      "epoch [180/50000] s_loss:164.7865 d_loss:-21.5452 val_s_loss:87.6253 val_d_loss:-27.4670 cost:23.40\n",
      "epoch [181/50000] s_loss:121.9240 d_loss:-26.5937 val_s_loss:51.4111 val_d_loss:-6.3291 cost:22.44\n",
      "epoch [182/50000] s_loss:78.2079 d_loss:-18.0187 val_s_loss:26.4708 val_d_loss:-5.7978 cost:22.21\n",
      "epoch [183/50000] s_loss:36.7413 d_loss:-19.3633 val_s_loss:5.2522 val_d_loss:-7.6199 cost:22.07\n",
      "epoch [184/50000] s_loss:22.2732 d_loss:-21.6710 val_s_loss:1.0196 val_d_loss:-15.5520 cost:21.90\n",
      "epoch [185/50000] s_loss:-21.2736 d_loss:-28.9107 val_s_loss:-18.0340 val_d_loss:-14.8308 cost:23.70\n",
      "epoch [186/50000] s_loss:40.8855 d_loss:-15.9761 val_s_loss:13.5490 val_d_loss:-8.4747 cost:24.16\n",
      "epoch [187/50000] s_loss:-9.7662 d_loss:-18.4539 val_s_loss:-12.3675 val_d_loss:-10.3526 cost:22.75\n",
      "epoch [188/50000] s_loss:-38.9984 d_loss:-22.9398 val_s_loss:-23.4710 val_d_loss:-12.8193 cost:22.62\n",
      "epoch [189/50000] s_loss:205.3931 d_loss:-12.5700 val_s_loss:131.2759 val_d_loss:-31.6591 cost:22.74\n",
      "epoch [190/50000] s_loss:246.6884 d_loss:-11.0317 val_s_loss:139.4548 val_d_loss:-25.8774 cost:23.01\n",
      "epoch [191/50000] s_loss:313.5197 d_loss:-21.1852 val_s_loss:148.7476 val_d_loss:-6.7080 cost:22.26\n",
      "epoch [192/50000] s_loss:218.3643 d_loss:-36.2395 val_s_loss:103.5076 val_d_loss:-14.0602 cost:22.15\n",
      "epoch [193/50000] s_loss:51.1466 d_loss:-27.8691 val_s_loss:56.8634 val_d_loss:-49.3619 cost:22.19\n",
      "epoch [194/50000] s_loss:114.9600 d_loss:-33.2385 val_s_loss:167.3862 val_d_loss:-132.3486 cost:21.94\n",
      "epoch [195/50000] s_loss:183.6725 d_loss:-50.2298 val_s_loss:43.6751 val_d_loss:-9.4916 cost:24.01\n",
      "epoch [196/50000] s_loss:59.9048 d_loss:-24.1727 val_s_loss:25.3302 val_d_loss:-10.5580 cost:23.70\n",
      "epoch [197/50000] s_loss:176.2210 d_loss:-11.3904 val_s_loss:92.9017 val_d_loss:-13.7168 cost:22.20\n",
      "epoch [198/50000] s_loss:213.5205 d_loss:-13.0221 val_s_loss:102.9957 val_d_loss:-6.7590 cost:22.60\n",
      "epoch [199/50000] s_loss:3.1668 d_loss:-16.7374 val_s_loss:-5.2940 val_d_loss:-9.6529 cost:22.14\n",
      "epoch [200/50000] s_loss:-26.2418 d_loss:-21.0978 val_s_loss:-20.2456 val_d_loss:-14.0505 cost:22.90\n",
      "epoch [201/50000] s_loss:-54.4127 d_loss:-19.1385 val_s_loss:-31.8242 val_d_loss:-14.5854 cost:22.02\n",
      "epoch [202/50000] s_loss:-56.2457 d_loss:-18.6824 val_s_loss:-32.9388 val_d_loss:-17.3252 cost:22.27\n",
      "epoch [203/50000] s_loss:15.0869 d_loss:-21.0527 val_s_loss:-0.0832 val_d_loss:-9.0371 cost:22.21\n",
      "epoch [204/50000] s_loss:6.0884 d_loss:-17.0645 val_s_loss:-3.0897 val_d_loss:-3.3065 cost:21.83\n",
      "epoch [205/50000] s_loss:811.6312 d_loss:-163.5028 val_s_loss:505.0925 val_d_loss:-73.6180 cost:24.83\n",
      "epoch [206/50000] s_loss:299.8097 d_loss:-6.3792 val_s_loss:168.2981 val_d_loss:-33.1281 cost:23.51\n",
      "epoch [207/50000] s_loss:379.2433 d_loss:-10.0610 val_s_loss:194.3644 val_d_loss:-10.5582 cost:22.45\n",
      "epoch [208/50000] s_loss:384.0827 d_loss:-17.0581 val_s_loss:201.9919 val_d_loss:-13.4689 cost:22.22\n",
      "epoch [209/50000] s_loss:288.6411 d_loss:-14.0531 val_s_loss:170.2548 val_d_loss:-43.4064 cost:21.62\n",
      "epoch [210/50000] s_loss:222.9110 d_loss:-15.2751 val_s_loss:120.3202 val_d_loss:-31.1129 cost:23.43\n",
      "epoch [211/50000] s_loss:151.3481 d_loss:-16.8600 val_s_loss:222.5532 val_d_loss:-150.8809 cost:22.18\n",
      "epoch [212/50000] s_loss:58.1596 d_loss:-17.9457 val_s_loss:154.6714 val_d_loss:-138.0313 cost:21.70\n",
      "epoch [213/50000] s_loss:98.1262 d_loss:-19.7221 val_s_loss:65.8496 val_d_loss:-39.0270 cost:21.80\n",
      "epoch [214/50000] s_loss:195.9610 d_loss:-34.2212 val_s_loss:72.8722 val_d_loss:-7.3489 cost:21.97\n",
      "epoch [215/50000] s_loss:145.0068 d_loss:-37.1506 val_s_loss:109.0570 val_d_loss:-34.5610 cost:24.87\n",
      "epoch [216/50000] s_loss:121.0468 d_loss:-17.0063 val_s_loss:47.6911 val_d_loss:-10.3919 cost:23.75\n",
      "epoch [217/50000] s_loss:78.4289 d_loss:-17.6216 val_s_loss:25.6283 val_d_loss:1.6870 cost:22.83\n",
      "epoch [218/50000] s_loss:60.0532 d_loss:-18.1463 val_s_loss:15.9060 val_d_loss:-7.4007 cost:22.02\n",
      "epoch [219/50000] s_loss:46.1088 d_loss:-24.7400 val_s_loss:8.9135 val_d_loss:-9.7849 cost:22.42\n",
      "epoch [220/50000] s_loss:11.5511 d_loss:-26.5684 val_s_loss:-7.6165 val_d_loss:-11.7668 cost:22.87\n",
      "epoch [221/50000] s_loss:97.3123 d_loss:-17.7219 val_s_loss:38.2579 val_d_loss:-17.0413 cost:21.62\n",
      "epoch [222/50000] s_loss:52.3054 d_loss:-23.1028 val_s_loss:11.3482 val_d_loss:-8.9054 cost:21.63\n",
      "epoch [223/50000] s_loss:44.0263 d_loss:-14.2271 val_s_loss:21.0306 val_d_loss:-11.9097 cost:22.06\n",
      "epoch [224/50000] s_loss:177.4839 d_loss:-18.7513 val_s_loss:85.8076 val_d_loss:-14.6413 cost:21.92\n",
      "epoch [225/50000] s_loss:-47.6944 d_loss:-21.4366 val_s_loss:-23.7935 val_d_loss:-11.4133 cost:24.75\n",
      "epoch [226/50000] s_loss:144.1106 d_loss:-26.8907 val_s_loss:56.3307 val_d_loss:11.0434 cost:23.56\n",
      "epoch [227/50000] s_loss:140.6725 d_loss:-23.0936 val_s_loss:76.9557 val_d_loss:-19.9078 cost:22.39\n",
      "epoch [228/50000] s_loss:101.2870 d_loss:-21.4795 val_s_loss:29.7596 val_d_loss:-19.4944 cost:21.93\n",
      "epoch [229/50000] s_loss:25.8272 d_loss:-30.4347 val_s_loss:-9.0893 val_d_loss:-10.0390 cost:22.34\n",
      "epoch [230/50000] s_loss:-9.6025 d_loss:-30.6158 val_s_loss:-19.5338 val_d_loss:-14.9976 cost:22.66\n",
      "epoch [231/50000] s_loss:-31.8280 d_loss:-25.4293 val_s_loss:-35.8945 val_d_loss:-11.9928 cost:22.02\n",
      "epoch [232/50000] s_loss:3.1085 d_loss:-20.3991 val_s_loss:33.1395 val_d_loss:-49.4574 cost:21.53\n",
      "epoch [233/50000] s_loss:22.2859 d_loss:-24.7492 val_s_loss:-2.1264 val_d_loss:-12.8134 cost:22.23\n",
      "epoch [234/50000] s_loss:-2.8756 d_loss:-26.6273 val_s_loss:-16.1310 val_d_loss:-9.6079 cost:22.04\n",
      "epoch [235/50000] s_loss:-10.2207 d_loss:-21.2434 val_s_loss:-15.9686 val_d_loss:-9.8097 cost:25.09\n",
      "epoch [236/50000] s_loss:118.5859 d_loss:-23.7799 val_s_loss:48.5039 val_d_loss:5.7063 cost:24.46\n",
      "epoch [237/50000] s_loss:30.8120 d_loss:-25.7574 val_s_loss:5.4688 val_d_loss:-12.9452 cost:23.18\n",
      "epoch [238/50000] s_loss:-14.1198 d_loss:-22.5309 val_s_loss:-11.1307 val_d_loss:-12.8289 cost:22.56\n",
      "epoch [239/50000] s_loss:31.8628 d_loss:-24.6264 val_s_loss:7.2472 val_d_loss:-6.5743 cost:22.54\n",
      "epoch [240/50000] s_loss:225.0455 d_loss:-42.9390 val_s_loss:132.0218 val_d_loss:-18.8619 cost:23.17\n",
      "epoch [241/50000] s_loss:51.6231 d_loss:-7.5315 val_s_loss:15.5543 val_d_loss:-3.6655 cost:21.56\n",
      "epoch [242/50000] s_loss:66.3073 d_loss:-18.8148 val_s_loss:13.5492 val_d_loss:-1.7118 cost:21.73\n",
      "epoch [243/50000] s_loss:2.6174 d_loss:-22.1980 val_s_loss:27.2418 val_d_loss:-45.1311 cost:22.17\n",
      "epoch [244/50000] s_loss:-80.3040 d_loss:-22.9265 val_s_loss:-51.9313 val_d_loss:-10.5475 cost:22.08\n",
      "epoch [245/50000] s_loss:-110.9996 d_loss:-20.1305 val_s_loss:-56.4011 val_d_loss:-10.6935 cost:24.96\n",
      "epoch [246/50000] s_loss:388.5157 d_loss:2.5297 val_s_loss:190.6824 val_d_loss:4.0219 cost:24.21\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    start = time.time()\n",
    "    ######## GAN ################\n",
    "    one = torch.FloatTensor([1])\n",
    "    mone = one * -1\n",
    "    \n",
    "    one = one.cuda()\n",
    "    mone = mone.cuda()\n",
    "    \n",
    "    one = one.mean()\n",
    "    mone = mone.mean()\n",
    "    ## ==== GAN --> D =====\n",
    "    for i in range(3):\n",
    "        for index, img in enumerate(train_loader):\n",
    "            AE.eval(), S.train(), D.train()\n",
    "\n",
    "            img = Variable(img).cuda()\n",
    "\n",
    "            # ====== AE ======\n",
    "            blur_image = AE(img)\n",
    "\n",
    "            _bs, _c, _w, _h = blur_image.shape\n",
    "            noise = torch.zeros(_bs, 1, _w, _h )\n",
    "            noise = noise + (0.01**0.5)*torch.randn(_bs, 1, _w, _h)\n",
    "            noise = noise.cuda()\n",
    "\n",
    "            blur_image_with_noise = torch.cat([blur_image, noise], 1)\n",
    "            fake_image = S(blur_image_with_noise) # 當成是 residual\n",
    "            \n",
    "            fake_image = fake_image + blur_image # blur image + residual\n",
    "            \n",
    "            fake_pair = torch.cat([img, fake_image], 1)\n",
    "            real_pair = torch.cat([img, img[torch.randperm(img.size(0)), :, :, :]], 1) if UPSET else torch.cat([img, img], 1)\n",
    "            # ====== Train D ======\n",
    "            for p in D.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "            optimizer_AE.zero_grad()\n",
    "            optimizer_S.zero_grad()\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "\n",
    "            real_D = D(real_pair)\n",
    "            real_D = real_D.mean()\n",
    "            real_D.backward(mone)\n",
    "\n",
    "\n",
    "            fake_D = D(fake_pair)\n",
    "            fake_D = fake_D.mean()\n",
    "            fake_D.backward(one)\n",
    "\n",
    "            gradient_penalty = calc_gradient_penalty(D, real_pair, fake_pair)\n",
    "            gradient_penalty.backward()\n",
    "\n",
    "            cost_D = fake_D - real_D + gradient_penalty\n",
    "            Wasserstein_D = real_D - fake_D\n",
    "            optimizer_D.step()\n",
    "    \n",
    "    ## ==== GAN --> G =====\n",
    "    for index, img in enumerate(train_loader):\n",
    "        AE.eval(), S.train(), D.train()\n",
    "\n",
    "        img = Variable(img).cuda()\n",
    "        # ======AE======\n",
    "        blur_image = AE(img)\n",
    "\n",
    "        _bs, _c, _w, _h = blur_image.shape\n",
    "        noise = torch.zeros(_bs, 1, _w, _h )\n",
    "        noise = noise + (0.01**0.5)*torch.randn(_bs, 1, _w, _h)\n",
    "        noise = noise.cuda()\n",
    "\n",
    "        blur_image_with_noise = torch.cat([blur_image, noise], 1)\n",
    "        fake_image = S(blur_image_with_noise)\n",
    "        \n",
    "        fake_image = fake_image + blur_image\n",
    "        \n",
    "        fake_pair = torch.cat([img, fake_image], 1)\n",
    "        # ====== Train G ======\n",
    "        for p in D.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        G_L1 = L1_loss(img, fake_image)\n",
    "        G = D(fake_pair)\n",
    "        G = G.mean()\n",
    "        G = G + G_L1 * ( G / G_L1)\n",
    "        \n",
    "        optimizer_AE.zero_grad()\n",
    "        optimizer_S.zero_grad()\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        G.backward(mone)\n",
    "        \n",
    "        cost_G = -G\n",
    "        optimizer_S.step()\n",
    "        \n",
    "    \n",
    "    # validation set\n",
    "    for index, val_img in enumerate(val_loader):\n",
    "        AE.eval(), S.eval(), D.eval()\n",
    "\n",
    "        val_img = Variable(val_img).cuda()\n",
    "        # ======AE======\n",
    "        val_blur_image = AE(val_img)\n",
    "\n",
    "        _bs, _c, _w, _h = val_blur_image.shape\n",
    "        noise = torch.zeros(_bs, 1, _w, _h )\n",
    "        noise = noise + (0.01**0.5)*torch.randn(_bs, 1, _w, _h)\n",
    "        noise = noise.cuda()\n",
    "\n",
    "        val_blur_image_with_noise = torch.cat([val_blur_image, noise], 1)\n",
    "\n",
    "        val_fake_image = S(val_blur_image_with_noise)       \n",
    "        val_fake_image = val_fake_image + val_blur_image\n",
    "        \n",
    "        \n",
    "        val_fake_pair = torch.cat([val_img, val_fake_image], 1)\n",
    "        val_real_pair = torch.cat([val_img, val_img[torch.randperm(val_img.size(0)), :, :, :]], 1) if UPSET else torch.cat([val_img, val_img], 1)\n",
    "        \n",
    "        val_real_D = D(val_real_pair)\n",
    "        val_real_D = val_real_D.mean()\n",
    "        \n",
    "        val_fake_D = D(val_fake_pair)\n",
    "        val_fake_D = val_fake_D.mean()\n",
    "        \n",
    "        val_gradient_penalty = calc_gradient_penalty(D, val_real_pair, val_fake_pair)\n",
    "        \n",
    "        val_G_L1 = L1_loss(val_img, val_fake_image)\n",
    "        # =========== Losses =========\n",
    "        val_Wasserstein_D = val_real_D - val_fake_D\n",
    "        \n",
    "        val_cost_G = -val_fake_D\n",
    "        val_cost_D = val_fake_D - val_real_D + val_gradient_penalty\n",
    "    \n",
    "    # evaluate\n",
    "    test_total_AUC = 0\n",
    "    test_total_AUC2 = 0\n",
    "    test_total_image = 0\n",
    "\n",
    "    for index, (test_img, mask) in enumerate(test_loader):\n",
    "        AE.eval(), S.eval(), D.eval()\n",
    "        test_img = Variable(test_img).cuda()\n",
    "        test_blur_image = AE(test_img)\n",
    "\n",
    "        _bs, _c, _w, _h = test_blur_image.shape\n",
    "        noise = torch.zeros(_bs, 1, _w, _h )\n",
    "        noise = noise + (0.01**0.5)*torch.randn(_bs, 1, _w, _h)\n",
    "        noise = noise.cuda()\n",
    "\n",
    "        test_blur_image_with_noise = torch.cat([test_blur_image, noise], 1)\n",
    "\n",
    "        test_fake_image = S(test_blur_image_with_noise)       \n",
    "        test_fake_image = test_fake_image + test_blur_image\n",
    "\n",
    "        # 計算 dif (相似度以及 L2)\n",
    "        dif, _ = perceptual_loss.forward(test_fake_image, test_img)\n",
    "        l2Dif = L2_loss(test_fake_image, test_img)\n",
    "        l2Dif = torch.mean(l2Dif, 1, True)\n",
    "        \n",
    "        pred_mask2 = difNormalize(dif)\n",
    "        pred_mask2 = torch.flatten(pred_mask2[0])\n",
    "        \n",
    "        pred_mask = difNormalize(dif[0] * l2Dif[0])\n",
    "        pred_mask = torch.flatten(pred_mask)\n",
    "        \n",
    "        mask = torch.mean(mask, 1, True)\n",
    "        true_mask = mask[0].cpu().detach().numpy().flatten()\n",
    "        true_mask = true_mask.astype(int)\n",
    "\n",
    "        AUC = roc_auc_score(true_mask, pred_mask.cpu().detach().numpy())\n",
    "        AUC2 = roc_auc_score(true_mask, pred_mask2.cpu().detach().numpy())\n",
    "\n",
    "        test_total_AUC += AUC\n",
    "        test_total_AUC2 += AUC2\n",
    "        test_total_image += 1\n",
    "    \n",
    "    # =================== GAN log========================\n",
    "    end = time.time()\n",
    "    print('epoch [{}/{}] s_loss:{:.4f} d_loss:{:.4f} val_s_loss:{:.4f} val_d_loss:{:.4f} cost:{:.2f}'.format(epoch+1, num_epochs, cost_G.item(), cost_D.item(), val_cost_G.item(), val_cost_D.item(), end-start ))\n",
    "    writer.add_scalars('eval', {\n",
    "        \"auc_roc_score\": test_total_AUC / test_total_image,\n",
    "        \"auc_roc_score(w/o L2)\": test_total_AUC2 / test_total_image,\n",
    "    }, epoch)\n",
    "    \n",
    "    writer.add_scalars('loss', {\n",
    "        \"Wasserstein Distance\": Wasserstein_D.item(),\n",
    "        \"Val Wasserstein Distance\": val_Wasserstein_D.item(),\n",
    "        \"gradient penalty\": gradient_penalty,\n",
    "        \"val gradient penalty\": val_gradient_penalty\n",
    "    }, epoch)\n",
    "    \n",
    "    writer.add_scalars('gan loss', {\n",
    "        \"l1_loss\": G_L1.item(),\n",
    "        \"g_loss\": cost_G.item(),\n",
    "        \"d_loss\": cost_D.item(),\n",
    "        \"val_l1_loss\": val_G_L1.item(),\n",
    "        \"val_g_loss\": val_cost_G.item(),\n",
    "        \"val_d_loss\": val_cost_D.item()\n",
    "    }, epoch)\n",
    "\n",
    "    writer.add_images('Blur', blur_image, epoch)\n",
    "    writer.add_images('Reconstruct', fake_image, epoch)\n",
    "    writer.add_images('Origin', img, epoch)\n",
    "\n",
    "    writer.add_images('Val Blur', val_blur_image, epoch)\n",
    "    writer.add_images('Val Reconstruct', val_fake_image, epoch)\n",
    "    writer.add_images('Val Origin', val_img, epoch)\n",
    "\n",
    "\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        if not os.path.exists('./save_weight/{}'.format(expName)):\n",
    "            os.makedirs('./save_weight/{}'.format(expName))\n",
    "        torch.save(S.state_dict(), './save_weight/{}/S_{}.npy'.format(expName, epoch))\n",
    "        torch.save(D.state_dict(), './save_weight/{}/D_{}.npy'.format(expName, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.eval()\n",
    "S.eval()\n",
    "for index, img in enumerate(test_loader):\n",
    "    test_img = Variable(img[0]).cuda()\n",
    "\n",
    "    # ======AE======\n",
    "    blur_image = AE(test_img)\n",
    "    \n",
    "    noise = torch.zeros(blur_image.shape[0], 1, blur_image.shape[2], blur_image.shape[3] )\n",
    "    noise = noise + (0.01**0.5)*torch.randn(blur_image.shape[0], 1, blur_image.shape[2], blur_image.shape[3])\n",
    "    noise = noise.cuda()\n",
    "    blur_image_with_noise = torch.cat([blur_image, noise], 1)\n",
    "    fake_image = S(blur_image_with_noise)\n",
    "    \n",
    "    \n",
    "    vutils.save_image(fake_image[0], './test_result/{}_simulated.png'.format(index))\n",
    "    vutils.save_image(blur_image[0], './test_result/{}_blur.png'.format(index))\n",
    "    vutils.save_image(test_img, './test_result/{}_origin.png'.format(index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
