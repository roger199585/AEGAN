{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from utils.tools import get_config, default_loader, is_image_file, normalize\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "sys.path.append('../PerceptualSimilarity')\n",
    "import models as PerceptualSimilarity\n",
    "\n",
    "# personal library\n",
    "from networks import autoencoder, simulator, discriminator\n",
    "from dataloader import MVTecDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 限制可以使用的 GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER parameters\n",
    "num_epochs = 50000\n",
    "batch_size = 32\n",
    "val_batch_size = 4\n",
    "ae_lr = 1e-4\n",
    "s_lr = 5e-4\n",
    "d_lr = 5e-4\n",
    "weight_decay = 1e-5\n",
    "UPSET=True\n",
    "expName = 'AEGAN-exp(wood + L1 + gradient)'\n",
    "writer = SummaryWriter('checkpoints/'+expName)\n",
    "TYPE='wood'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDatset = MVTecDataset.MVTecDataset(TYPE=TYPE, isTrain='train')\n",
    "testDatset = MVTecDataset.MVTecDataset(TYPE=TYPE, isTrain='test')\n",
    "valDataset = MVTecDataset.MVTecDataset(TYPE=TYPE, isTrain='val')\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=valDataset,\n",
    "    batch_size=val_batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset=trainDatset,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=testDatset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Perceptual loss...\n",
      "Loading model from: /root/AFS/Corn/AEGAN/PerceptualSimilarity/models/weights/v0.1/alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "AE = autoencoder.Autoencoder().cuda()\n",
    "S = nn.DataParallel(simulator.Simulator(3, 8)).cuda()\n",
    "D = nn.DataParallel(discriminator.Discriminator(6, 16)).cuda()\n",
    "\n",
    "# Loss\n",
    "L1_loss = nn.L1Loss()\n",
    "L2_loss = nn.MSELoss(reduction='none')\n",
    "perceptual_loss = PerceptualSimilarity.PerceptualLoss(model='net-lin', net='alex', use_gpu=True, gpu_ids=[0])\n",
    "\n",
    "# Optimizer\n",
    "optimizer_AE = torch.optim.Adam(\n",
    "    AE.parameters(), \n",
    "    lr=ae_lr,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "optimizer_S = torch.optim.Adam(\n",
    "    S.parameters(), \n",
    "    lr=s_lr,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "optimizer_D = torch.optim.Adam(\n",
    "    D.parameters(), \n",
    "    lr=d_lr,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.load_state_dict(torch.load('./save_weight/AE-wood-z-2x2-exp2/AE_2500.npy', map_location=\"cuda:0\"), False)\n",
    "\n",
    "AE = nn.DataParallel(AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Solve: RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED\n",
    "torch.backends.cudnn.enabled = False \n",
    "\n",
    "# 拿掉煩人的 warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "\n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    # print \"real_data: \", real_data.size(), fake_data.size()\n",
    "    BATCH_SIZE = real_data.size(0)\n",
    "    alpha = torch.rand(BATCH_SIZE, 1)\n",
    "    alpha = alpha.expand(BATCH_SIZE, real_data.nelement()//BATCH_SIZE).contiguous().view(BATCH_SIZE, 6, 256, 256)\n",
    "    alpha = alpha.cuda()\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = autograd.grad(\n",
    "        outputs=disc_interpolates, \n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "        create_graph=True, \n",
    "        retain_graph=True, \n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "    return gradient_penalty\n",
    "\n",
    "def gradient_loss(gen_frames, gt_frames, alpha=1):\n",
    "    def gradient(x):\n",
    "        h_x = x.size()[-2]\n",
    "        w_x = x.size()[-1]\n",
    "        # gradient step=1\n",
    "        left = x\n",
    "        right = F.pad(x, [0, 1, 0, 0])[:, :, :, 1:]\n",
    "        top = x\n",
    "        bottom = F.pad(x, [0, 0, 0, 1])[:, :, 1:, :]\n",
    "\n",
    "        # dx, dy = torch.abs(right - left), torch.abs(bottom - top)\n",
    "        dx, dy = right - left, bottom - top \n",
    "        # dx will always have zeros in the last column, right-left\n",
    "        # dy will always have zeros in the last row,    bottom-top\n",
    "        dx[:, :, :, -1] = 0\n",
    "        dy[:, :, -1, :] = 0\n",
    "\n",
    "        return dx, dy\n",
    "\n",
    "    # gradient\n",
    "    gen_dx, gen_dy = gradient(gen_frames)\n",
    "    gt_dx, gt_dy = gradient(gt_frames)\n",
    "    #\n",
    "    grad_diff_x = torch.abs(gt_dx - gen_dx)\n",
    "    grad_diff_y = torch.abs(gt_dy - gen_dy)\n",
    "\n",
    "    # condense into one tensor and avg\n",
    "    return torch.mean(grad_diff_x ** alpha + grad_diff_y ** alpha)\n",
    "\n",
    "def difNormalize(input_matrix, threshold=None):\n",
    "    _min = torch.min(input_matrix)\n",
    "    _max = torch.max(input_matrix)\n",
    "    \n",
    "    input_matrix = (input_matrix - _min) / (_max - _min)\n",
    "    \n",
    "    if threshold != None:\n",
    "        input_matrix[input_matrix < threshold] = 0\n",
    "        input_matrix[input_matrix >= threshold] = 1\n",
    "        \n",
    "    return input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/50000] s_loss:1912.1523 d_loss:-278.1466 val_s_loss:1213.3286 val_d_loss:-225.7886 cost:26.74\n",
      "epoch [2/50000] s_loss:1849.9199 d_loss:-181.2305 val_s_loss:795.5139 val_d_loss:59.6611 cost:22.13\n",
      "epoch [3/50000] s_loss:1016.2395 d_loss:-187.1540 val_s_loss:545.4919 val_d_loss:-85.8004 cost:21.34\n",
      "epoch [4/50000] s_loss:1561.7660 d_loss:-222.8959 val_s_loss:690.3129 val_d_loss:-88.0677 cost:21.55\n",
      "epoch [5/50000] s_loss:1429.7407 d_loss:-333.0352 val_s_loss:735.8066 val_d_loss:-208.0760 cost:22.57\n",
      "epoch [6/50000] s_loss:1331.9973 d_loss:-287.2206 val_s_loss:1054.0688 val_d_loss:-572.6839 cost:25.44\n",
      "epoch [7/50000] s_loss:814.6937 d_loss:-209.0937 val_s_loss:1078.2573 val_d_loss:-625.2091 cost:23.04\n",
      "epoch [8/50000] s_loss:859.4924 d_loss:-127.0710 val_s_loss:437.9287 val_d_loss:-43.8189 cost:24.72\n",
      "epoch [9/50000] s_loss:742.3251 d_loss:-111.7458 val_s_loss:341.1018 val_d_loss:-36.7675 cost:23.39\n",
      "epoch [10/50000] s_loss:593.0656 d_loss:-60.3526 val_s_loss:236.5579 val_d_loss:-31.5240 cost:24.40\n",
      "epoch [11/50000] s_loss:354.0166 d_loss:-41.0577 val_s_loss:121.7842 val_d_loss:6.6650 cost:23.30\n",
      "epoch [12/50000] s_loss:387.1466 d_loss:-55.2326 val_s_loss:345.5479 val_d_loss:-163.2977 cost:22.29\n",
      "epoch [13/50000] s_loss:279.6465 d_loss:-44.5201 val_s_loss:80.2243 val_d_loss:0.6966 cost:23.02\n",
      "epoch [14/50000] s_loss:226.8045 d_loss:-22.4608 val_s_loss:484.4774 val_d_loss:-352.8288 cost:26.04\n",
      "epoch [15/50000] s_loss:438.5782 d_loss:-57.3177 val_s_loss:219.6617 val_d_loss:0.4148 cost:24.84\n",
      "epoch [16/50000] s_loss:253.1870 d_loss:-34.6464 val_s_loss:90.3143 val_d_loss:13.6431 cost:27.19\n",
      "epoch [17/50000] s_loss:146.8407 d_loss:-30.7833 val_s_loss:44.8258 val_d_loss:-1.7942 cost:25.00\n",
      "epoch [18/50000] s_loss:159.1551 d_loss:-23.8266 val_s_loss:758.5753 val_d_loss:-529.0577 cost:24.40\n",
      "epoch [19/50000] s_loss:21.7688 d_loss:-8.4499 val_s_loss:260.6613 val_d_loss:-251.5120 cost:23.89\n",
      "epoch [20/50000] s_loss:59.4799 d_loss:-11.3363 val_s_loss:70.3951 val_d_loss:-44.6648 cost:24.64\n",
      "epoch [21/50000] s_loss:79.7731 d_loss:-13.8677 val_s_loss:23.5178 val_d_loss:3.9645 cost:22.49\n",
      "epoch [22/50000] s_loss:167.5346 d_loss:-15.3188 val_s_loss:126.3325 val_d_loss:-62.2047 cost:21.91\n",
      "epoch [23/50000] s_loss:159.5382 d_loss:-14.9261 val_s_loss:99.3496 val_d_loss:-34.6518 cost:22.09\n",
      "epoch [24/50000] s_loss:170.7272 d_loss:-20.2888 val_s_loss:236.3721 val_d_loss:-136.3603 cost:21.83\n",
      "epoch [25/50000] s_loss:168.1726 d_loss:-31.5769 val_s_loss:40.9041 val_d_loss:2.4349 cost:23.31\n",
      "epoch [26/50000] s_loss:167.9879 d_loss:-28.6450 val_s_loss:41.5507 val_d_loss:-0.8674 cost:24.71\n",
      "epoch [27/50000] s_loss:158.3933 d_loss:-24.3475 val_s_loss:119.0083 val_d_loss:-37.5901 cost:22.79\n",
      "epoch [28/50000] s_loss:89.3998 d_loss:-16.2616 val_s_loss:15.7876 val_d_loss:6.6793 cost:22.90\n",
      "epoch [29/50000] s_loss:82.2198 d_loss:-32.7253 val_s_loss:386.2399 val_d_loss:-278.6655 cost:22.10\n",
      "epoch [30/50000] s_loss:80.8228 d_loss:-19.2177 val_s_loss:20.3752 val_d_loss:0.5884 cost:22.83\n",
      "epoch [31/50000] s_loss:64.5842 d_loss:-8.7070 val_s_loss:27.5081 val_d_loss:0.3762 cost:22.62\n",
      "epoch [32/50000] s_loss:107.0596 d_loss:-5.2594 val_s_loss:45.6615 val_d_loss:-3.2119 cost:21.79\n",
      "epoch [33/50000] s_loss:171.6450 d_loss:-10.9931 val_s_loss:71.9001 val_d_loss:3.2136 cost:21.50\n",
      "epoch [34/50000] s_loss:178.3441 d_loss:-31.3055 val_s_loss:109.8156 val_d_loss:-30.6491 cost:22.39\n",
      "epoch [35/50000] s_loss:148.5441 d_loss:-15.6285 val_s_loss:52.2254 val_d_loss:0.3462 cost:23.72\n",
      "epoch [36/50000] s_loss:87.2755 d_loss:-15.3196 val_s_loss:241.6881 val_d_loss:-149.5246 cost:24.35\n",
      "epoch [37/50000] s_loss:72.9394 d_loss:-18.4935 val_s_loss:188.1335 val_d_loss:-126.9753 cost:22.66\n",
      "epoch [38/50000] s_loss:55.5669 d_loss:-17.1547 val_s_loss:8.5839 val_d_loss:11.7615 cost:22.20\n",
      "epoch [39/50000] s_loss:58.5326 d_loss:-13.9409 val_s_loss:19.3326 val_d_loss:-4.3672 cost:22.53\n",
      "epoch [40/50000] s_loss:56.1091 d_loss:-13.5571 val_s_loss:470.1238 val_d_loss:-376.7501 cost:22.98\n",
      "epoch [41/50000] s_loss:107.2207 d_loss:-19.7182 val_s_loss:28.7073 val_d_loss:1.9708 cost:22.55\n",
      "epoch [42/50000] s_loss:28.8979 d_loss:-17.4049 val_s_loss:273.6103 val_d_loss:-243.9209 cost:22.05\n",
      "epoch [43/50000] s_loss:15.9755 d_loss:-25.9189 val_s_loss:232.1094 val_d_loss:-209.7819 cost:22.23\n",
      "epoch [44/50000] s_loss:-40.6919 d_loss:-8.8334 val_s_loss:-42.6542 val_d_loss:3.1167 cost:21.97\n",
      "epoch [45/50000] s_loss:93.5491 d_loss:-14.1778 val_s_loss:42.1704 val_d_loss:-15.5880 cost:24.27\n",
      "epoch [46/50000] s_loss:103.8308 d_loss:-18.7759 val_s_loss:78.5328 val_d_loss:-42.4595 cost:24.06\n",
      "epoch [47/50000] s_loss:68.7223 d_loss:-12.0596 val_s_loss:15.0996 val_d_loss:3.2091 cost:22.47\n",
      "epoch [48/50000] s_loss:60.7716 d_loss:-13.1222 val_s_loss:32.6101 val_d_loss:-22.8257 cost:22.96\n",
      "epoch [49/50000] s_loss:35.2321 d_loss:-15.8923 val_s_loss:17.9473 val_d_loss:-21.8638 cost:22.12\n",
      "epoch [50/50000] s_loss:6.7718 d_loss:-9.3215 val_s_loss:19.9375 val_d_loss:-28.7752 cost:23.01\n",
      "epoch [51/50000] s_loss:20.9481 d_loss:-12.7215 val_s_loss:222.5616 val_d_loss:-206.4250 cost:22.29\n",
      "epoch [52/50000] s_loss:31.7537 d_loss:-13.6606 val_s_loss:115.0050 val_d_loss:-83.6710 cost:21.78\n",
      "epoch [53/50000] s_loss:47.9639 d_loss:-13.7482 val_s_loss:42.4780 val_d_loss:-39.9042 cost:21.89\n",
      "epoch [54/50000] s_loss:-58.0731 d_loss:-21.9159 val_s_loss:-55.0754 val_d_loss:2.2852 cost:22.58\n",
      "epoch [55/50000] s_loss:4.6795 d_loss:-21.7235 val_s_loss:-19.0585 val_d_loss:-1.4899 cost:24.40\n",
      "epoch [56/50000] s_loss:28.6270 d_loss:-16.4132 val_s_loss:-9.7972 val_d_loss:3.3947 cost:23.42\n",
      "epoch [57/50000] s_loss:-74.2056 d_loss:-20.3277 val_s_loss:98.6120 val_d_loss:-135.5143 cost:22.53\n",
      "epoch [58/50000] s_loss:-159.8577 d_loss:-20.3092 val_s_loss:155.5562 val_d_loss:-210.3726 cost:22.11\n",
      "epoch [59/50000] s_loss:-7.4634 d_loss:-18.2162 val_s_loss:-15.3063 val_d_loss:-0.3765 cost:22.59\n",
      "epoch [60/50000] s_loss:115.5428 d_loss:-17.7210 val_s_loss:48.8699 val_d_loss:-9.5234 cost:23.42\n",
      "epoch [61/50000] s_loss:69.5174 d_loss:-18.7120 val_s_loss:0.8514 val_d_loss:3.8859 cost:22.13\n",
      "epoch [62/50000] s_loss:3.1663 d_loss:-6.8137 val_s_loss:104.2999 val_d_loss:-95.6541 cost:22.39\n",
      "epoch [63/50000] s_loss:-8.3907 d_loss:-15.2916 val_s_loss:18.0797 val_d_loss:-39.4920 cost:22.61\n",
      "epoch [64/50000] s_loss:105.7208 d_loss:-9.2639 val_s_loss:71.9477 val_d_loss:-30.1698 cost:22.34\n",
      "epoch [65/50000] s_loss:80.1136 d_loss:-17.8887 val_s_loss:24.5698 val_d_loss:-5.7237 cost:25.50\n",
      "epoch [66/50000] s_loss:-10.0685 d_loss:-17.9196 val_s_loss:441.1919 val_d_loss:-304.4235 cost:23.65\n",
      "epoch [67/50000] s_loss:-112.0561 d_loss:-11.6642 val_s_loss:-69.2382 val_d_loss:-2.8423 cost:23.01\n",
      "epoch [68/50000] s_loss:-33.9410 d_loss:-15.2491 val_s_loss:-47.7140 val_d_loss:9.0626 cost:22.59\n",
      "epoch [69/50000] s_loss:78.8951 d_loss:-18.7673 val_s_loss:36.7710 val_d_loss:-1.9788 cost:23.45\n",
      "epoch [70/50000] s_loss:42.7218 d_loss:-18.9815 val_s_loss:-1.7235 val_d_loss:-4.7524 cost:22.85\n",
      "epoch [71/50000] s_loss:-47.4612 d_loss:-20.3670 val_s_loss:155.8638 val_d_loss:-140.2857 cost:22.27\n",
      "epoch [72/50000] s_loss:31.7793 d_loss:-22.6851 val_s_loss:132.3690 val_d_loss:-105.5710 cost:21.90\n",
      "epoch [73/50000] s_loss:-59.2528 d_loss:-17.9722 val_s_loss:-45.2264 val_d_loss:-6.9453 cost:22.55\n",
      "epoch [74/50000] s_loss:-151.3248 d_loss:-27.0918 val_s_loss:-71.0984 val_d_loss:-26.0604 cost:24.51\n",
      "epoch [75/50000] s_loss:676.3608 d_loss:-26.9849 val_s_loss:417.2393 val_d_loss:-74.3235 cost:24.92\n",
      "epoch [76/50000] s_loss:376.2579 d_loss:-5.3162 val_s_loss:176.6280 val_d_loss:3.6279 cost:23.19\n",
      "epoch [77/50000] s_loss:741.3622 d_loss:-18.2552 val_s_loss:340.5717 val_d_loss:6.8803 cost:22.92\n",
      "epoch [78/50000] s_loss:420.8403 d_loss:-33.7676 val_s_loss:189.7498 val_d_loss:4.7896 cost:23.61\n",
      "epoch [79/50000] s_loss:246.5937 d_loss:-21.5429 val_s_loss:157.5973 val_d_loss:10.0160 cost:25.21\n",
      "epoch [80/50000] s_loss:704.6963 d_loss:-149.8627 val_s_loss:740.7036 val_d_loss:-384.5767 cost:23.61\n",
      "epoch [81/50000] s_loss:506.7666 d_loss:-91.7080 val_s_loss:485.2719 val_d_loss:-228.8690 cost:23.82\n",
      "epoch [82/50000] s_loss:1035.2241 d_loss:-92.0676 val_s_loss:1259.9380 val_d_loss:-693.6823 cost:22.90\n",
      "epoch [83/50000] s_loss:403.0278 d_loss:-2.3101 val_s_loss:110.3132 val_d_loss:47.7968 cost:22.67\n",
      "epoch [84/50000] s_loss:303.6847 d_loss:-42.7702 val_s_loss:114.4266 val_d_loss:14.6963 cost:25.95\n",
      "epoch [85/50000] s_loss:440.5963 d_loss:-58.5732 val_s_loss:196.1018 val_d_loss:-15.6846 cost:23.88\n",
      "epoch [86/50000] s_loss:397.4300 d_loss:-12.7950 val_s_loss:180.2557 val_d_loss:7.8563 cost:23.49\n",
      "epoch [87/50000] s_loss:556.4113 d_loss:-22.4466 val_s_loss:265.6145 val_d_loss:1.7197 cost:22.88\n",
      "epoch [88/50000] s_loss:659.7595 d_loss:-17.3563 val_s_loss:301.2504 val_d_loss:8.1992 cost:22.13\n",
      "epoch [89/50000] s_loss:510.7324 d_loss:-23.9911 val_s_loss:232.2327 val_d_loss:3.7236 cost:23.51\n",
      "epoch [90/50000] s_loss:624.9515 d_loss:-17.7369 val_s_loss:269.3938 val_d_loss:5.3742 cost:22.14\n",
      "epoch [91/50000] s_loss:592.7599 d_loss:-21.6389 val_s_loss:347.1452 val_d_loss:-68.7258 cost:22.62\n",
      "epoch [92/50000] s_loss:600.8770 d_loss:-39.0956 val_s_loss:301.2943 val_d_loss:-19.3729 cost:21.98\n",
      "epoch [93/50000] s_loss:568.6472 d_loss:-25.3816 val_s_loss:390.8327 val_d_loss:-90.4779 cost:23.23\n",
      "epoch [94/50000] s_loss:102.6778 d_loss:-13.5766 val_s_loss:121.1004 val_d_loss:-76.6290 cost:26.04\n",
      "epoch [95/50000] s_loss:162.1059 d_loss:-13.8169 val_s_loss:105.4213 val_d_loss:-38.3426 cost:23.92\n",
      "epoch [96/50000] s_loss:253.3977 d_loss:-20.0999 val_s_loss:346.0618 val_d_loss:-227.0160 cost:23.09\n",
      "epoch [97/50000] s_loss:328.3089 d_loss:-18.8897 val_s_loss:329.8478 val_d_loss:-178.0971 cost:23.97\n",
      "epoch [98/50000] s_loss:343.9946 d_loss:-25.1553 val_s_loss:333.0941 val_d_loss:-161.1721 cost:24.42\n",
      "epoch [99/50000] s_loss:391.3064 d_loss:-28.0942 val_s_loss:804.4083 val_d_loss:-554.1543 cost:23.38\n",
      "epoch [100/50000] s_loss:431.1556 d_loss:-30.8823 val_s_loss:1120.7927 val_d_loss:-747.7999 cost:23.26\n",
      "epoch [101/50000] s_loss:449.4124 d_loss:-64.8146 val_s_loss:286.4954 val_d_loss:-122.6567 cost:22.63\n",
      "epoch [102/50000] s_loss:206.8678 d_loss:-38.1258 val_s_loss:64.5457 val_d_loss:5.2452 cost:22.14\n",
      "epoch [103/50000] s_loss:137.0945 d_loss:-21.6337 val_s_loss:43.5152 val_d_loss:0.1904 cost:24.40\n",
      "epoch [104/50000] s_loss:207.6827 d_loss:-30.1806 val_s_loss:80.6904 val_d_loss:-2.1803 cost:25.41\n",
      "epoch [105/50000] s_loss:108.9535 d_loss:-16.9657 val_s_loss:53.5293 val_d_loss:-16.1662 cost:22.99\n",
      "epoch [106/50000] s_loss:149.7471 d_loss:-20.6753 val_s_loss:58.5910 val_d_loss:1.9335 cost:22.40\n",
      "epoch [107/50000] s_loss:225.5493 d_loss:-18.3474 val_s_loss:89.6812 val_d_loss:6.0752 cost:22.31\n",
      "epoch [108/50000] s_loss:194.0027 d_loss:-19.9297 val_s_loss:75.7469 val_d_loss:1.7389 cost:24.40\n",
      "epoch [109/50000] s_loss:235.1000 d_loss:-14.0175 val_s_loss:101.2995 val_d_loss:1.1425 cost:22.33\n",
      "epoch [110/50000] s_loss:165.8503 d_loss:-16.7832 val_s_loss:68.5376 val_d_loss:-2.9132 cost:22.51\n",
      "epoch [111/50000] s_loss:200.9500 d_loss:-18.2336 val_s_loss:81.5736 val_d_loss:-2.3752 cost:22.38\n",
      "epoch [112/50000] s_loss:250.3036 d_loss:-23.2428 val_s_loss:109.1604 val_d_loss:-1.3721 cost:22.39\n",
      "epoch [113/50000] s_loss:291.6559 d_loss:-19.6671 val_s_loss:134.1528 val_d_loss:-18.3633 cost:22.99\n",
      "epoch [114/50000] s_loss:346.4494 d_loss:-28.3747 val_s_loss:1096.4894 val_d_loss:-726.0131 cost:25.15\n",
      "epoch [115/50000] s_loss:361.4785 d_loss:-31.4733 val_s_loss:297.1305 val_d_loss:-106.9451 cost:22.65\n",
      "epoch [116/50000] s_loss:284.3578 d_loss:-10.2488 val_s_loss:123.7524 val_d_loss:3.5300 cost:22.19\n",
      "epoch [117/50000] s_loss:221.0355 d_loss:-16.4847 val_s_loss:99.0709 val_d_loss:-2.0888 cost:22.78\n",
      "epoch [118/50000] s_loss:436.2989 d_loss:-40.7356 val_s_loss:237.8035 val_d_loss:-16.3673 cost:23.37\n",
      "epoch [119/50000] s_loss:350.0710 d_loss:-8.7485 val_s_loss:169.0517 val_d_loss:-0.5702 cost:21.83\n",
      "epoch [120/50000] s_loss:329.0436 d_loss:-14.1162 val_s_loss:164.6045 val_d_loss:-6.9027 cost:22.18\n",
      "epoch [121/50000] s_loss:266.7538 d_loss:-23.4842 val_s_loss:113.6096 val_d_loss:2.0342 cost:21.99\n",
      "epoch [122/50000] s_loss:243.5613 d_loss:-13.8940 val_s_loss:142.7410 val_d_loss:-29.8909 cost:21.80\n",
      "epoch [123/50000] s_loss:211.7466 d_loss:-4.1244 val_s_loss:111.4516 val_d_loss:-20.9033 cost:25.45\n",
      "epoch [124/50000] s_loss:188.6913 d_loss:-12.2185 val_s_loss:84.3117 val_d_loss:-2.8016 cost:25.07\n",
      "epoch [125/50000] s_loss:160.8978 d_loss:-12.2157 val_s_loss:66.0269 val_d_loss:-0.7754 cost:23.68\n",
      "epoch [126/50000] s_loss:128.2163 d_loss:-21.4233 val_s_loss:156.3335 val_d_loss:-104.7650 cost:22.48\n",
      "epoch [127/50000] s_loss:109.0173 d_loss:-17.4821 val_s_loss:65.6557 val_d_loss:-20.4667 cost:22.53\n",
      "epoch [128/50000] s_loss:63.9058 d_loss:-11.5146 val_s_loss:26.8096 val_d_loss:-5.1491 cost:23.93\n",
      "epoch [129/50000] s_loss:89.8239 d_loss:-15.2086 val_s_loss:27.2055 val_d_loss:-2.0973 cost:22.26\n",
      "epoch [130/50000] s_loss:129.5822 d_loss:-22.8518 val_s_loss:42.1441 val_d_loss:-2.1111 cost:21.98\n",
      "epoch [131/50000] s_loss:108.9951 d_loss:-19.9530 val_s_loss:52.0180 val_d_loss:-9.7714 cost:21.82\n",
      "epoch [132/50000] s_loss:124.9981 d_loss:-17.7208 val_s_loss:42.1520 val_d_loss:-4.3443 cost:21.93\n",
      "epoch [133/50000] s_loss:113.5744 d_loss:-21.9038 val_s_loss:39.5248 val_d_loss:-10.6652 cost:25.28\n",
      "epoch [134/50000] s_loss:133.3317 d_loss:-14.4481 val_s_loss:53.5481 val_d_loss:1.8864 cost:24.05\n",
      "epoch [135/50000] s_loss:141.0753 d_loss:-11.1577 val_s_loss:102.4948 val_d_loss:-39.4721 cost:22.09\n",
      "epoch [136/50000] s_loss:426.4274 d_loss:-16.3117 val_s_loss:206.8553 val_d_loss:1.9741 cost:22.55\n",
      "epoch [137/50000] s_loss:262.7864 d_loss:-17.0044 val_s_loss:114.7179 val_d_loss:-2.1580 cost:22.45\n",
      "epoch [138/50000] s_loss:178.2806 d_loss:-21.0389 val_s_loss:125.0392 val_d_loss:-36.4334 cost:23.20\n",
      "epoch [139/50000] s_loss:169.8840 d_loss:-14.0875 val_s_loss:66.7914 val_d_loss:-2.1644 cost:22.34\n",
      "epoch [140/50000] s_loss:148.9093 d_loss:-19.8392 val_s_loss:56.9179 val_d_loss:-1.1484 cost:22.37\n",
      "epoch [141/50000] s_loss:96.0456 d_loss:-10.2853 val_s_loss:34.8484 val_d_loss:-2.1268 cost:22.14\n",
      "epoch [142/50000] s_loss:127.0154 d_loss:-16.9357 val_s_loss:106.2338 val_d_loss:-51.0793 cost:21.34\n",
      "epoch [143/50000] s_loss:97.0091 d_loss:-24.6892 val_s_loss:30.5257 val_d_loss:-4.4484 cost:24.35\n",
      "epoch [144/50000] s_loss:100.6501 d_loss:-24.7194 val_s_loss:33.5057 val_d_loss:-9.1068 cost:24.00\n",
      "epoch [145/50000] s_loss:112.6692 d_loss:-21.5341 val_s_loss:36.9849 val_d_loss:-9.9523 cost:22.33\n",
      "epoch [146/50000] s_loss:87.7028 d_loss:-23.3809 val_s_loss:80.0355 val_d_loss:-44.5434 cost:22.81\n",
      "epoch [147/50000] s_loss:62.5080 d_loss:-17.1562 val_s_loss:7.5220 val_d_loss:3.3848 cost:22.25\n",
      "epoch [148/50000] s_loss:84.3327 d_loss:-15.3788 val_s_loss:33.4525 val_d_loss:-6.0229 cost:23.47\n",
      "epoch [149/50000] s_loss:53.1590 d_loss:-15.4266 val_s_loss:13.9976 val_d_loss:-2.1894 cost:22.73\n",
      "epoch [150/50000] s_loss:103.6680 d_loss:-17.8548 val_s_loss:16.6828 val_d_loss:-8.7486 cost:22.05\n",
      "epoch [151/50000] s_loss:39.6876 d_loss:-22.8198 val_s_loss:2.4519 val_d_loss:1.2430 cost:21.77\n",
      "epoch [152/50000] s_loss:48.7110 d_loss:-15.1661 val_s_loss:13.3320 val_d_loss:-8.3898 cost:21.62\n",
      "epoch [153/50000] s_loss:-2.9417 d_loss:-16.3884 val_s_loss:-14.5736 val_d_loss:-0.4812 cost:24.27\n",
      "epoch [154/50000] s_loss:-57.8949 d_loss:-22.1020 val_s_loss:-41.9891 val_d_loss:0.7791 cost:23.19\n",
      "epoch [155/50000] s_loss:240.4390 d_loss:-8.2825 val_s_loss:103.9502 val_d_loss:-3.4022 cost:23.05\n",
      "epoch [156/50000] s_loss:383.1420 d_loss:-14.7614 val_s_loss:179.4309 val_d_loss:7.7605 cost:22.24\n",
      "epoch [157/50000] s_loss:359.8618 d_loss:-6.8200 val_s_loss:167.7935 val_d_loss:4.3294 cost:21.87\n",
      "epoch [158/50000] s_loss:423.7806 d_loss:-11.1744 val_s_loss:224.4428 val_d_loss:-33.9973 cost:23.56\n",
      "epoch [159/50000] s_loss:385.2531 d_loss:-15.3631 val_s_loss:424.9319 val_d_loss:-236.2449 cost:22.30\n",
      "epoch [160/50000] s_loss:173.1244 d_loss:-17.9500 val_s_loss:77.5093 val_d_loss:-2.0511 cost:21.79\n",
      "epoch [161/50000] s_loss:76.9587 d_loss:-11.8997 val_s_loss:27.1278 val_d_loss:4.8503 cost:22.15\n",
      "epoch [162/50000] s_loss:-23.0972 d_loss:-13.8066 val_s_loss:-11.2856 val_d_loss:-15.3973 cost:21.49\n",
      "epoch [163/50000] s_loss:-120.3509 d_loss:-17.1430 val_s_loss:293.8862 val_d_loss:-273.4794 cost:24.34\n",
      "epoch [164/50000] s_loss:-104.5344 d_loss:-17.0978 val_s_loss:100.5908 val_d_loss:-125.5924 cost:23.59\n",
      "epoch [165/50000] s_loss:-134.1997 d_loss:-21.1487 val_s_loss:-89.6803 val_d_loss:-1.8048 cost:22.45\n",
      "epoch [166/50000] s_loss:-189.9613 d_loss:-22.0247 val_s_loss:-113.3657 val_d_loss:2.0144 cost:21.62\n",
      "epoch [167/50000] s_loss:210.4938 d_loss:-28.1531 val_s_loss:305.1623 val_d_loss:-161.0684 cost:23.37\n",
      "epoch [168/50000] s_loss:208.2466 d_loss:-20.8430 val_s_loss:126.6710 val_d_loss:-28.0284 cost:22.55\n",
      "epoch [169/50000] s_loss:65.9562 d_loss:-22.6490 val_s_loss:17.7058 val_d_loss:-4.0687 cost:22.04\n",
      "epoch [170/50000] s_loss:-44.5538 d_loss:-17.5325 val_s_loss:-41.8750 val_d_loss:-5.8327 cost:22.14\n",
      "epoch [171/50000] s_loss:-19.1300 d_loss:-5.3250 val_s_loss:-26.3658 val_d_loss:6.1316 cost:22.15\n",
      "epoch [172/50000] s_loss:-48.7368 d_loss:-9.7101 val_s_loss:110.5515 val_d_loss:-133.2933 cost:22.13\n",
      "epoch [173/50000] s_loss:-78.6660 d_loss:-14.9324 val_s_loss:-45.9199 val_d_loss:-5.0437 cost:25.17\n",
      "epoch [174/50000] s_loss:26.4045 d_loss:-18.3376 val_s_loss:-11.9025 val_d_loss:8.7704 cost:23.30\n",
      "epoch [175/50000] s_loss:66.2176 d_loss:-9.1729 val_s_loss:211.2717 val_d_loss:-175.7040 cost:22.37\n",
      "epoch [176/50000] s_loss:169.2831 d_loss:-17.1246 val_s_loss:122.4554 val_d_loss:-50.0221 cost:22.82\n",
      "epoch [177/50000] s_loss:75.9848 d_loss:-13.4521 val_s_loss:17.3920 val_d_loss:-3.6938 cost:22.90\n",
      "epoch [178/50000] s_loss:-104.7814 d_loss:-16.3664 val_s_loss:24.4480 val_d_loss:-59.9574 cost:22.41\n",
      "epoch [179/50000] s_loss:89.5608 d_loss:-8.0973 val_s_loss:206.8672 val_d_loss:-148.3560 cost:22.70\n",
      "epoch [180/50000] s_loss:71.0451 d_loss:-16.4908 val_s_loss:41.8664 val_d_loss:-19.9393 cost:21.97\n",
      "epoch [181/50000] s_loss:54.1136 d_loss:-14.9666 val_s_loss:16.9861 val_d_loss:5.7793 cost:21.58\n",
      "epoch [182/50000] s_loss:4.0492 d_loss:-22.6215 val_s_loss:22.9212 val_d_loss:-27.0558 cost:23.46\n",
      "epoch [183/50000] s_loss:-4.5489 d_loss:-18.8663 val_s_loss:-13.0808 val_d_loss:6.9568 cost:25.39\n",
      "epoch [184/50000] s_loss:144.5122 d_loss:-7.3879 val_s_loss:63.1866 val_d_loss:8.2355 cost:22.58\n",
      "epoch [185/50000] s_loss:105.8815 d_loss:-9.8132 val_s_loss:48.4917 val_d_loss:1.2933 cost:22.68\n",
      "epoch [186/50000] s_loss:277.3036 d_loss:-15.3315 val_s_loss:126.3246 val_d_loss:1.0997 cost:22.75\n",
      "epoch [187/50000] s_loss:219.6598 d_loss:-18.4565 val_s_loss:95.8174 val_d_loss:-3.7464 cost:23.34\n",
      "epoch [188/50000] s_loss:151.2293 d_loss:-17.9678 val_s_loss:54.8851 val_d_loss:1.3491 cost:22.81\n",
      "epoch [189/50000] s_loss:-12.5356 d_loss:-19.5885 val_s_loss:-2.8313 val_d_loss:-23.2869 cost:22.03\n",
      "epoch [190/50000] s_loss:44.1066 d_loss:-25.0409 val_s_loss:5.1415 val_d_loss:-10.4989 cost:22.19\n",
      "epoch [191/50000] s_loss:72.8136 d_loss:-18.2932 val_s_loss:27.2084 val_d_loss:-12.7093 cost:21.99\n",
      "epoch [192/50000] s_loss:151.2152 d_loss:-2.3737 val_s_loss:89.1363 val_d_loss:-11.4608 cost:23.11\n",
      "epoch [193/50000] s_loss:78.9320 d_loss:-12.7867 val_s_loss:36.5112 val_d_loss:-11.1267 cost:24.45\n",
      "epoch [194/50000] s_loss:161.6610 d_loss:-17.5722 val_s_loss:71.5692 val_d_loss:-8.2901 cost:22.63\n",
      "epoch [195/50000] s_loss:196.9850 d_loss:-19.1959 val_s_loss:311.5917 val_d_loss:-211.2984 cost:22.82\n",
      "epoch [196/50000] s_loss:188.0494 d_loss:-10.4814 val_s_loss:147.8884 val_d_loss:-55.1682 cost:22.15\n",
      "epoch [197/50000] s_loss:102.7829 d_loss:-13.7770 val_s_loss:37.7092 val_d_loss:-6.7363 cost:23.38\n",
      "epoch [198/50000] s_loss:17.6090 d_loss:-21.9170 val_s_loss:-7.7704 val_d_loss:-9.9606 cost:22.27\n",
      "epoch [199/50000] s_loss:-106.4717 d_loss:-9.6548 val_s_loss:-45.3904 val_d_loss:24.7051 cost:22.22\n",
      "epoch [200/50000] s_loss:450.9327 d_loss:-178.4379 val_s_loss:1413.3944 val_d_loss:-1125.7277 cost:22.14\n",
      "epoch [201/50000] s_loss:114.6380 d_loss:0.7417 val_s_loss:128.4893 val_d_loss:-71.8403 cost:21.89\n",
      "epoch [202/50000] s_loss:363.6813 d_loss:-18.1714 val_s_loss:221.7681 val_d_loss:-65.2418 cost:23.22\n",
      "epoch [203/50000] s_loss:557.0659 d_loss:-18.3015 val_s_loss:288.0267 val_d_loss:-57.7215 cost:24.62\n",
      "epoch [204/50000] s_loss:435.4665 d_loss:-33.8111 val_s_loss:187.9729 val_d_loss:7.8569 cost:22.41\n",
      "epoch [205/50000] s_loss:396.8104 d_loss:-37.9389 val_s_loss:147.6456 val_d_loss:17.0686 cost:21.88\n",
      "epoch [206/50000] s_loss:318.7015 d_loss:-36.0174 val_s_loss:132.9559 val_d_loss:10.0280 cost:22.23\n",
      "epoch [207/50000] s_loss:260.8395 d_loss:-24.8853 val_s_loss:274.0597 val_d_loss:-144.4323 cost:23.03\n",
      "epoch [208/50000] s_loss:191.6109 d_loss:-21.7833 val_s_loss:185.4760 val_d_loss:-57.3233 cost:21.74\n",
      "epoch [209/50000] s_loss:244.1604 d_loss:-15.3332 val_s_loss:473.6649 val_d_loss:-327.1005 cost:22.10\n",
      "epoch [210/50000] s_loss:310.8822 d_loss:-28.0611 val_s_loss:305.3900 val_d_loss:-143.2020 cost:22.10\n",
      "epoch [211/50000] s_loss:332.0495 d_loss:-11.7983 val_s_loss:163.8379 val_d_loss:-4.5073 cost:21.58\n",
      "epoch [212/50000] s_loss:308.8642 d_loss:-25.3008 val_s_loss:676.9724 val_d_loss:-499.1927 cost:22.99\n",
      "epoch [213/50000] s_loss:342.6501 d_loss:-18.2303 val_s_loss:430.0023 val_d_loss:-246.2738 cost:24.26\n",
      "epoch [214/50000] s_loss:387.1736 d_loss:-22.2787 val_s_loss:182.8432 val_d_loss:1.5400 cost:22.97\n",
      "epoch [215/50000] s_loss:218.8058 d_loss:-13.5575 val_s_loss:106.9734 val_d_loss:-6.3006 cost:22.80\n",
      "epoch [216/50000] s_loss:278.8292 d_loss:-15.0479 val_s_loss:137.8403 val_d_loss:-4.7766 cost:22.55\n",
      "epoch [217/50000] s_loss:424.8798 d_loss:-21.1922 val_s_loss:216.0756 val_d_loss:-17.1148 cost:23.32\n",
      "epoch [218/50000] s_loss:337.3414 d_loss:-21.7011 val_s_loss:141.1487 val_d_loss:-0.4003 cost:22.41\n",
      "epoch [219/50000] s_loss:369.6279 d_loss:-25.7189 val_s_loss:180.4720 val_d_loss:-12.1751 cost:21.95\n",
      "epoch [220/50000] s_loss:319.9749 d_loss:-20.3181 val_s_loss:129.1486 val_d_loss:-3.8246 cost:22.20\n",
      "epoch [221/50000] s_loss:254.6615 d_loss:-13.1828 val_s_loss:107.3183 val_d_loss:-0.9530 cost:22.03\n",
      "epoch [222/50000] s_loss:255.1391 d_loss:-19.1713 val_s_loss:119.7063 val_d_loss:-0.2602 cost:23.55\n",
      "epoch [223/50000] s_loss:227.5623 d_loss:-17.3450 val_s_loss:106.1532 val_d_loss:-7.6341 cost:24.23\n",
      "epoch [224/50000] s_loss:234.0365 d_loss:-17.6291 val_s_loss:96.7160 val_d_loss:-4.4939 cost:23.30\n",
      "epoch [225/50000] s_loss:224.7006 d_loss:-19.3363 val_s_loss:103.8158 val_d_loss:-4.3350 cost:22.96\n",
      "epoch [226/50000] s_loss:232.2594 d_loss:-17.3829 val_s_loss:106.4625 val_d_loss:-4.3372 cost:21.82\n",
      "epoch [227/50000] s_loss:194.5409 d_loss:-11.4340 val_s_loss:75.4168 val_d_loss:-11.7349 cost:23.37\n",
      "epoch [228/50000] s_loss:196.0194 d_loss:-14.9855 val_s_loss:127.1092 val_d_loss:-31.0462 cost:21.65\n",
      "epoch [229/50000] s_loss:194.2511 d_loss:-15.1416 val_s_loss:81.1537 val_d_loss:8.5911 cost:21.75\n",
      "epoch [230/50000] s_loss:151.9745 d_loss:-15.4886 val_s_loss:61.3869 val_d_loss:-1.3579 cost:22.48\n",
      "epoch [231/50000] s_loss:205.5083 d_loss:-14.7286 val_s_loss:88.0713 val_d_loss:-6.1061 cost:21.79\n",
      "epoch [232/50000] s_loss:168.0860 d_loss:-14.2179 val_s_loss:74.8652 val_d_loss:-2.2713 cost:23.84\n",
      "epoch [233/50000] s_loss:168.7435 d_loss:-11.2082 val_s_loss:75.7554 val_d_loss:-5.9677 cost:25.82\n",
      "epoch [234/50000] s_loss:152.9042 d_loss:-12.9920 val_s_loss:66.0204 val_d_loss:1.7187 cost:23.42\n",
      "epoch [235/50000] s_loss:172.8907 d_loss:-15.2579 val_s_loss:88.4758 val_d_loss:-8.9401 cost:22.93\n",
      "epoch [236/50000] s_loss:189.7363 d_loss:-16.0379 val_s_loss:82.5182 val_d_loss:-2.0928 cost:22.60\n",
      "epoch [237/50000] s_loss:149.6127 d_loss:-12.5932 val_s_loss:72.0356 val_d_loss:-4.7637 cost:22.85\n",
      "epoch [238/50000] s_loss:150.5181 d_loss:-11.5892 val_s_loss:71.6572 val_d_loss:-7.0283 cost:22.15\n",
      "epoch [239/50000] s_loss:192.3120 d_loss:-15.4486 val_s_loss:84.5963 val_d_loss:-5.6523 cost:22.07\n",
      "epoch [240/50000] s_loss:180.0891 d_loss:-12.6293 val_s_loss:85.3102 val_d_loss:-12.9647 cost:21.91\n",
      "epoch [241/50000] s_loss:44.5047 d_loss:-4.6549 val_s_loss:18.4403 val_d_loss:-0.8245 cost:21.71\n",
      "epoch [242/50000] s_loss:23.2298 d_loss:-6.4383 val_s_loss:9.0871 val_d_loss:-2.3477 cost:24.07\n",
      "epoch [243/50000] s_loss:80.5140 d_loss:-10.7255 val_s_loss:35.0524 val_d_loss:-5.0387 cost:25.77\n",
      "epoch [244/50000] s_loss:184.9364 d_loss:-17.6557 val_s_loss:91.7985 val_d_loss:-6.0055 cost:23.27\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    start = time.time()\n",
    "    ######## GAN ################\n",
    "    one = torch.FloatTensor([1])\n",
    "    mone = one * -1\n",
    "    \n",
    "    one = one.cuda()\n",
    "    mone = mone.cuda()\n",
    "    \n",
    "    one = one.mean()\n",
    "    mone = mone.mean()\n",
    "    ## ==== GAN --> D =====\n",
    "    for i in range(3):\n",
    "        for index, img in enumerate(train_loader):\n",
    "            AE.eval(), S.train(), D.train()\n",
    "\n",
    "            img = Variable(img).cuda()\n",
    "\n",
    "            # ====== AE ======\n",
    "            blur_image = AE(img)\n",
    "\n",
    "            _bs, _c, _w, _h = blur_image.shape\n",
    "            noise = torch.zeros(_bs, 1, _w, _h )\n",
    "            noise = noise + (0.01**0.5)*torch.randn(_bs, 1, _w, _h)\n",
    "            noise = noise.cuda()\n",
    "\n",
    "            blur_image_with_noise = torch.cat([blur_image, noise], 1)\n",
    "            fake_image = S(blur_image_with_noise) # 當成是 residual\n",
    "            \n",
    "            fake_image = fake_image + blur_image # blur image + residual\n",
    "            \n",
    "            fake_pair = torch.cat([img, fake_image], 1)\n",
    "            real_pair = torch.cat([img, img[torch.randperm(img.size(0)), :, :, :]], 1) if UPSET else torch.cat([img, img], 1)\n",
    "            # ====== Train D ======\n",
    "            for p in D.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "            optimizer_AE.zero_grad()\n",
    "            optimizer_S.zero_grad()\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "\n",
    "            real_D = D(real_pair)\n",
    "            real_D = real_D.mean()\n",
    "            real_D.backward(mone)\n",
    "\n",
    "\n",
    "            fake_D = D(fake_pair)\n",
    "            fake_D = fake_D.mean()\n",
    "            fake_D.backward(one)\n",
    "\n",
    "            gradient_penalty = calc_gradient_penalty(D, real_pair, fake_pair)\n",
    "            gradient_penalty.backward()\n",
    "\n",
    "            cost_D = fake_D - real_D + gradient_penalty\n",
    "            Wasserstein_D = real_D - fake_D\n",
    "            optimizer_D.step()\n",
    "    \n",
    "    ## ==== GAN --> G =====\n",
    "    for index, img in enumerate(train_loader):\n",
    "        AE.eval(), S.train(), D.train()\n",
    "\n",
    "        img = Variable(img).cuda()\n",
    "        # ======AE======\n",
    "        blur_image = AE(img)\n",
    "\n",
    "        _bs, _c, _w, _h = blur_image.shape\n",
    "        noise = torch.zeros(_bs, 1, _w, _h )\n",
    "        noise = noise + (0.01**0.5)*torch.randn(_bs, 1, _w, _h)\n",
    "        noise = noise.cuda()\n",
    "\n",
    "        blur_image_with_noise = torch.cat([blur_image, noise], 1)\n",
    "        fake_image = S(blur_image_with_noise)\n",
    "        \n",
    "        fake_image = fake_image + blur_image\n",
    "        \n",
    "        fake_pair = torch.cat([img, fake_image], 1)\n",
    "        # ====== Train G ======\n",
    "        for p in D.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        G_L1 = L1_loss(img, fake_image)\n",
    "        grad_loss = gradient_loss(img, fake_image)\n",
    "        G = D(fake_pair)\n",
    "        G = G.mean()\n",
    "        G = G + G_L1 * ( G / G_L1) + grad_loss\n",
    "        \n",
    "        optimizer_AE.zero_grad()\n",
    "        optimizer_S.zero_grad()\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        G.backward(mone)\n",
    "        \n",
    "        cost_G = -G\n",
    "        optimizer_S.step()\n",
    "        \n",
    "    \n",
    "    # validation set\n",
    "    for index, val_img in enumerate(val_loader):\n",
    "        AE.eval(), S.eval(), D.eval()\n",
    "\n",
    "        val_img = Variable(val_img).cuda()\n",
    "        # ======AE======\n",
    "        val_blur_image = AE(val_img)\n",
    "\n",
    "        _bs, _c, _w, _h = val_blur_image.shape\n",
    "        noise = torch.zeros(_bs, 1, _w, _h )\n",
    "        noise = noise + (0.01**0.5)*torch.randn(_bs, 1, _w, _h)\n",
    "        noise = noise.cuda()\n",
    "\n",
    "        val_blur_image_with_noise = torch.cat([val_blur_image, noise], 1)\n",
    "\n",
    "        val_fake_image = S(val_blur_image_with_noise)       \n",
    "        val_fake_image = val_fake_image + val_blur_image\n",
    "        \n",
    "        \n",
    "        val_fake_pair = torch.cat([val_img, val_fake_image], 1)\n",
    "        val_real_pair = torch.cat([val_img, val_img[torch.randperm(val_img.size(0)), :, :, :]], 1) if UPSET else torch.cat([val_img, val_img], 1)\n",
    "        \n",
    "        val_real_D = D(val_real_pair)\n",
    "        val_real_D = val_real_D.mean()\n",
    "        \n",
    "        val_fake_D = D(val_fake_pair)\n",
    "        val_fake_D = val_fake_D.mean()\n",
    "        \n",
    "        val_gradient_penalty = calc_gradient_penalty(D, val_real_pair, val_fake_pair)\n",
    "        \n",
    "        val_G_L1 = L1_loss(val_img, val_fake_image)\n",
    "        val_grad_loss = gradient_loss(val_img, val_fake_image)\n",
    "        # =========== Losses =========\n",
    "        val_Wasserstein_D = val_real_D - val_fake_D\n",
    "        \n",
    "        val_cost_G = -val_fake_D\n",
    "        val_cost_D = val_fake_D - val_real_D + val_gradient_penalty\n",
    "    \n",
    "    # evaluate\n",
    "    test_total_AUC = 0\n",
    "    test_total_AUC2 = 0\n",
    "    test_total_image = 0\n",
    "\n",
    "    for index, (test_img, mask) in enumerate(test_loader):\n",
    "        AE.eval(), S.eval(), D.eval()\n",
    "        test_img = Variable(test_img).cuda()\n",
    "        test_blur_image = AE(test_img)\n",
    "\n",
    "        _bs, _c, _w, _h = test_blur_image.shape\n",
    "        noise = torch.zeros(_bs, 1, _w, _h )\n",
    "        noise = noise + (0.01**0.5)*torch.randn(_bs, 1, _w, _h)\n",
    "        noise = noise.cuda()\n",
    "\n",
    "        test_blur_image_with_noise = torch.cat([test_blur_image, noise], 1)\n",
    "\n",
    "        test_fake_image = S(test_blur_image_with_noise)       \n",
    "        test_fake_image = test_fake_image + test_blur_image\n",
    "\n",
    "        # 計算 dif (相似度以及 L2)\n",
    "        dif, _ = perceptual_loss.forward(test_fake_image, test_img)\n",
    "        l2Dif = L2_loss(test_fake_image, test_img)\n",
    "        l2Dif = torch.mean(l2Dif, 1, True)\n",
    "        \n",
    "        pred_mask2 = difNormalize(dif)\n",
    "        pred_mask2 = torch.flatten(pred_mask2[0])\n",
    "        \n",
    "        pred_mask = difNormalize(dif[0] * l2Dif[0])\n",
    "        pred_mask = torch.flatten(pred_mask)\n",
    "        \n",
    "        mask = torch.mean(mask, 1, True)\n",
    "        true_mask = mask[0].cpu().detach().numpy().flatten()\n",
    "        true_mask = true_mask.astype(int)\n",
    "\n",
    "        AUC = roc_auc_score(true_mask, pred_mask.cpu().detach().numpy())\n",
    "        AUC2 = roc_auc_score(true_mask, pred_mask2.cpu().detach().numpy())\n",
    "\n",
    "        test_total_AUC += AUC\n",
    "        test_total_AUC2 += AUC2\n",
    "        test_total_image += 1\n",
    "    \n",
    "    # =================== GAN log========================\n",
    "    end = time.time()\n",
    "    print('epoch [{}/{}] s_loss:{:.4f} d_loss:{:.4f} val_s_loss:{:.4f} val_d_loss:{:.4f} cost:{:.2f}'.format(epoch+1, num_epochs, cost_G.item(), cost_D.item(), val_cost_G.item(), val_cost_D.item(), end-start ))\n",
    "    writer.add_scalars('eval', {\n",
    "        \"auc_roc_score\": test_total_AUC / test_total_image,\n",
    "        \"auc_roc_score(w/o L2)\": test_total_AUC2 / test_total_image,\n",
    "    }, epoch)\n",
    "    \n",
    "    writer.add_scalars('loss', {\n",
    "        \"Wasserstein Distance\": Wasserstein_D.item(),\n",
    "        \"Val Wasserstein Distance\": val_Wasserstein_D.item(),\n",
    "        \"gradient penalty\": gradient_penalty,\n",
    "        \"val gradient penalty\": val_gradient_penalty\n",
    "    }, epoch)\n",
    "    \n",
    "    writer.add_scalars('gan loss', {\n",
    "        \"l1_loss\": G_L1.item(),\n",
    "        \"g_loss\": cost_G.item(),\n",
    "        \"d_loss\": cost_D.item(),\n",
    "        \"gradient_loss\": grad_loss.item(),\n",
    "        \"val_l1_loss\": val_G_L1.item(),\n",
    "        \"val_g_loss\": val_cost_G.item(),\n",
    "        \"val_d_loss\": val_cost_D.item(),\n",
    "        \"val_gradient_loss\": val_grad_loss.item(),\n",
    "    }, epoch)\n",
    "\n",
    "    writer.add_images('Blur', blur_image, epoch)\n",
    "    writer.add_images('Reconstruct', fake_image, epoch)\n",
    "    writer.add_images('Origin', img, epoch)\n",
    "\n",
    "    writer.add_images('Val Blur', val_blur_image, epoch)\n",
    "    writer.add_images('Val Reconstruct', val_fake_image, epoch)\n",
    "    writer.add_images('Val Origin', val_img, epoch)\n",
    "\n",
    "\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        if not os.path.exists('./save_weight/{}'.format(expName)):\n",
    "            os.makedirs('./save_weight/{}'.format(expName))\n",
    "        torch.save(S.state_dict(), './save_weight/{}/S_{}.npy'.format(expName, epoch))\n",
    "        torch.save(D.state_dict(), './save_weight/{}/D_{}.npy'.format(expName, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.eval()\n",
    "S.eval()\n",
    "for index, img in enumerate(test_loader):\n",
    "    test_img = Variable(img[0]).cuda()\n",
    "\n",
    "    # ======AE======\n",
    "    blur_image = AE(test_img)\n",
    "    \n",
    "    noise = torch.zeros(blur_image.shape[0], 1, blur_image.shape[2], blur_image.shape[3] )\n",
    "    noise = noise + (0.01**0.5)*torch.randn(blur_image.shape[0], 1, blur_image.shape[2], blur_image.shape[3])\n",
    "    noise = noise.cuda()\n",
    "    blur_image_with_noise = torch.cat([blur_image, noise], 1)\n",
    "    fake_image = S(blur_image_with_noise)\n",
    "    \n",
    "    \n",
    "    vutils.save_image(fake_image[0], './test_result/{}_simulated.png'.format(index))\n",
    "    vutils.save_image(blur_image[0], './test_result/{}_blur.png'.format(index))\n",
    "    vutils.save_image(test_img, './test_result/{}_origin.png'.format(index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
